{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?name=Object+Detection+with+RetinaNet+on+Arthropods+dataset+%2F+training&download_url=https%3A%2F%2Fgithub.com%2Ftakumiohym%2Fpractical-ml-vision-book-ja%2Fraw%2Fmaster%2F04_detect_segment%2F04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "_nb_loc = \"04_detect_segment/04ab_retinanet_arthropods_train.ipynb\" # change to reflect your notebook\n",
    "_nb_title = \"Object Detection with RetinaNet on Arthropods dataset / training\" # change to reflect your notebook\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/takumiohym/practical-ml-vision-book-ja/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/>\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNetを使用したArthropodデータセットの物体検出: 学習\n",
    "このノートブックは、TPUまたはGPUで実行するように設定されています。GPUでは実行に時間がかかるため、特にTPUでの実行を推奨しています。以下はTPUv3での実行用に設定されていますので、**TPUv2(Colab)やGPUなど、メモリの少ないハードウェアで実行する場合は、以下の[設定]セクションでバッチサイズ(`BATCH_SIZE`)を小さく設定してください。**\n",
    "\n",
    "Vertex AI WorkbenchやColabでTPUを使用する方法については[README](https://github.com/takumiohym/practical-ml-vision-book-ja/blob/main/README.md)を参照してください。\n",
    "\n",
    "TPUでのトレーニングには、書き込み可能なプライベートGCSバケットが必要です。以下のGCSバケットのセクションを参照してください。\n",
    "\n",
    "このノートブックでは、Tensorflow Model GardenのRetinaNet実装を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tf-models-official==2.8\n",
    "# please restart the kernel after installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import time, re, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "\n",
    "# Tensorflow Model Garden imports\n",
    "import official as model_garden\n",
    "from official.vision.beta.configs import retinanet as retinanet_cfg\n",
    "from official.vision.beta.configs import backbones as backbones_cfg\n",
    "from official.vision.beta.serving import export_saved_model_lib\n",
    "from official.core import train_lib\n",
    "\n",
    "# TODO\n",
    "# load the backbone checkpoint from the official loacation as soon as it is published\n",
    "# save the model configuration to the saved_odel folder as per best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCSバケット  \n",
    "このバケットには以下が保存されます\n",
    " - 学習状況をモニタリングできるTensorboard用データ\n",
    " - モデルのCheckpointファイル\n",
    " - トレーニング後のSaved Modelファイル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your own GCS bucket here. GCS is required if training on TPU.\n",
    "# On GPU, a local folder will work.\n",
    "BUCKET='gs://YOUR_BUCKET_NAME' # Specify your GCS bucket name\n",
    "MODEL_ARTIFACT_BUCKET = f'{BUCKET}/arthropod_jobs/'\n",
    "MODEL_DIR = MODEL_ARTIFACT_BUCKET + str(int(time.time()))\n",
    "\n",
    "# If you are running on Colaboratory, you must authenticate\n",
    "# for Colab to have write access to the bucket.\n",
    "\n",
    "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
    "if IS_COLAB_BACKEND:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU/GPU検出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 08:04:00.921785: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-05-08 08:04:00.933298: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.112.92.178:8470}\n",
      "2022-05-08 08:04:00.933339: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34346}\n",
      "2022-05-08 08:04:00.949316: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.112.92.178:8470}\n",
      "2022-05-08 08:04:00.949363: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34346}\n",
      "2022-05-08 08:04:00.950060: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:437] Started server with target: grpc://localhost:34346\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tpu-direct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tpu-direct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "try: # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs or multi-GPU machines\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec'\n",
    "VALID_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec'\n",
    "SPINET_MOBILE_CHECKPOINT = 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/'\n",
    "\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 80\n",
    "\n",
    "RAW_CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera',\n",
    "               '_truncated', '_blurred', '_occluded', ]\n",
    "CLASSES = [klass for klass in RAW_CLASSES if klass not in ['_truncated', '_blurred', '_occluded']]\n",
    "\n",
    "# Lepidoptera = butterfies and moths\n",
    "# Hymenoptera = wasps, bees and ants\n",
    "# Hemiptera = true bugs (cicadas, aphids, shield bugs, ...)\n",
    "# Odonata = dragonflies\n",
    "# Diptera = fies\n",
    "# Araneae = spiders\n",
    "# Coleoptera = beetles\n",
    "\n",
    "# NOT IN DATASET\n",
    "# Orthoptera = grasshoppers\n",
    "\n",
    "print(\"Model dir:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データファイルをロードする  \n",
    "データセットはすでにTFRecord形式で準備されています。<br/>\n",
    "データの準備に使用したスクリプトは「04aa_retinanet_arthropods_dataprep.ipynb」を参照してください。<br/>\n",
    "TFRecordsファイルを手動で解析したり可視化したりする際は、「04ac_retinanet_arthropods_predict.ipynb」のコードを参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "    24 TFRecord files.\n",
      "    11544 images\n",
      "    Steps per epoch: 45\n",
      "\n",
      "Validation dataset:\n",
      "    8 TFRecord files.\n",
      "    3832 images\n",
      "    Validation steps: 14\n",
      "\n",
      "Global batch size: 256\n"
     ]
    }
   ],
   "source": [
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return int(np.sum(n))\n",
    "\n",
    "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
    "NB_TRAIN_IMAGES = count_data_items(TRAIN_FILENAMES)\n",
    "STEPS_PER_EPOCH = NB_TRAIN_IMAGES // BATCH_SIZE\n",
    "\n",
    "VALID_FILENAMES = tf.io.gfile.glob(VALID_DATA_PATH_PATTERN)\n",
    "NB_VALID_IMAGES = count_data_items(VALID_FILENAMES)\n",
    "VALID_STEPS = NB_VALID_IMAGES // BATCH_SIZE\n",
    "\n",
    "print(\"Training dataset:\")\n",
    "print(f\"    {len(TRAIN_FILENAMES)} TFRecord files.\")\n",
    "print(f\"    {NB_TRAIN_IMAGES} images\")\n",
    "print(\"    Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "print()\n",
    "print(\"Validation dataset:\")\n",
    "print(f\"    {len(VALID_FILENAMES)} TFRecord files.\")\n",
    "print(f\"    {NB_VALID_IMAGES} images\")\n",
    "print(\"    Validation steps:\", VALID_STEPS)\n",
    "print()\n",
    "print(\"Global batch size:\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runtime': {'all_reduce_alg': None,\n",
      "             'batchnorm_spatial_persistent': False,\n",
      "             'dataset_num_private_threads': None,\n",
      "             'default_shard_dim': -1,\n",
      "             'distribution_strategy': 'mirrored',\n",
      "             'enable_xla': False,\n",
      "             'gpu_thread_mode': None,\n",
      "             'loss_scale': None,\n",
      "             'mixed_precision_dtype': None,\n",
      "             'num_cores_per_replica': 1,\n",
      "             'num_gpus': 0,\n",
      "             'num_packs': 1,\n",
      "             'per_gpu_thread_count': 0,\n",
      "             'run_eagerly': False,\n",
      "             'task_index': -1,\n",
      "             'tpu': None,\n",
      "             'tpu_enable_xla_dynamic_padder': None,\n",
      "             'worker_hosts': None},\n",
      " 'task': {'annotation_file': None,\n",
      "          'export_config': {'cast_detection_classes_to_float': False,\n",
      "                            'cast_num_detections_to_float': False,\n",
      "                            'output_normalized_coordinates': False},\n",
      "          'init_checkpoint': 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/',\n",
      "          'init_checkpoint_modules': 'backbone',\n",
      "          'losses': {'box_loss_weight': 50,\n",
      "                     'focal_loss_alpha': 0.25,\n",
      "                     'focal_loss_gamma': 1.5,\n",
      "                     'huber_loss_delta': 0.1,\n",
      "                     'l2_weight_decay': 0.0,\n",
      "                     'loss_weight': 1.0},\n",
      "          'model': {'anchor': {'anchor_size': 4.0,\n",
      "                               'aspect_ratios': [0.5, 1.0, 2.0],\n",
      "                               'num_scales': 3},\n",
      "                    'backbone': {'spinenet_mobile': {'expand_ratio': 6,\n",
      "                                                     'max_level': 7,\n",
      "                                                     'min_level': 3,\n",
      "                                                     'model_id': '49',\n",
      "                                                     'se_ratio': 0.2,\n",
      "                                                     'stochastic_depth_drop_rate': 0.0,\n",
      "                                                     'use_keras_upsampling_2d': False},\n",
      "                                 'type': 'spinenet_mobile'},\n",
      "                    'decoder': {'fpn': {'fusion_type': 'sum',\n",
      "                                        'num_filters': 256,\n",
      "                                        'use_separable_conv': False},\n",
      "                                'type': 'fpn'},\n",
      "                    'detection_generator': {'apply_nms': True,\n",
      "                                            'max_num_detections': 100,\n",
      "                                            'nms_iou_threshold': 0.5,\n",
      "                                            'nms_version': 'v2',\n",
      "                                            'pre_nms_score_threshold': 0.05,\n",
      "                                            'pre_nms_top_k': 5000,\n",
      "                                            'soft_nms_sigma': None,\n",
      "                                            'use_cpu_nms': False},\n",
      "                    'head': {'attribute_heads': [],\n",
      "                             'num_convs': 4,\n",
      "                             'num_filters': 256,\n",
      "                             'use_separable_conv': False},\n",
      "                    'input_size': [384, 384, 3],\n",
      "                    'max_level': 7,\n",
      "                    'min_level': 3,\n",
      "                    'norm_activation': {'activation': 'relu',\n",
      "                                        'norm_epsilon': 0.001,\n",
      "                                        'norm_momentum': 0.99,\n",
      "                                        'use_sync_bn': True},\n",
      "                    'num_classes': 8},\n",
      "          'name': None,\n",
      "          'per_category_metrics': False,\n",
      "          'train_data': {'block_length': 1,\n",
      "                         'cache': False,\n",
      "                         'cycle_length': None,\n",
      "                         'decoder': {'simple_decoder': {'mask_binarize_threshold': None,\n",
      "                                                        'regenerate_source_id': False},\n",
      "                                     'type': 'simple_decoder'},\n",
      "                         'deterministic': None,\n",
      "                         'drop_remainder': True,\n",
      "                         'dtype': 'bfloat16',\n",
      "                         'enable_tf_data_service': False,\n",
      "                         'file_type': 'tfrecord',\n",
      "                         'global_batch_size': 256,\n",
      "                         'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec',\n",
      "                         'is_training': True,\n",
      "                         'parser': {'aug_policy': None,\n",
      "                                    'aug_rand_hflip': True,\n",
      "                                    'aug_scale_max': 2.0,\n",
      "                                    'aug_scale_min': 0.7,\n",
      "                                    'aug_type': None,\n",
      "                                    'match_threshold': 0.5,\n",
      "                                    'max_num_instances': 100,\n",
      "                                    'num_channels': 3,\n",
      "                                    'skip_crowd_during_training': True,\n",
      "                                    'unmatched_threshold': 0.5},\n",
      "                         'seed': None,\n",
      "                         'sharding': True,\n",
      "                         'shuffle_buffer_size': 10000,\n",
      "                         'tf_data_service_address': None,\n",
      "                         'tf_data_service_job_name': None,\n",
      "                         'tfds_as_supervised': False,\n",
      "                         'tfds_data_dir': '',\n",
      "                         'tfds_name': '',\n",
      "                         'tfds_skip_decoding_feature': '',\n",
      "                         'tfds_split': ''},\n",
      "          'validation_data': {'block_length': 1,\n",
      "                              'cache': False,\n",
      "                              'cycle_length': None,\n",
      "                              'decoder': {'simple_decoder': {'mask_binarize_threshold': None,\n",
      "                                                             'regenerate_source_id': False},\n",
      "                                          'type': 'simple_decoder'},\n",
      "                              'deterministic': None,\n",
      "                              'drop_remainder': True,\n",
      "                              'dtype': 'bfloat16',\n",
      "                              'enable_tf_data_service': False,\n",
      "                              'file_type': 'tfrecord',\n",
      "                              'global_batch_size': 256,\n",
      "                              'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec',\n",
      "                              'is_training': False,\n",
      "                              'parser': {'aug_policy': None,\n",
      "                                         'aug_rand_hflip': False,\n",
      "                                         'aug_scale_max': 1.0,\n",
      "                                         'aug_scale_min': 1.0,\n",
      "                                         'aug_type': None,\n",
      "                                         'match_threshold': 0.5,\n",
      "                                         'max_num_instances': 100,\n",
      "                                         'num_channels': 3,\n",
      "                                         'skip_crowd_during_training': True,\n",
      "                                         'unmatched_threshold': 0.5},\n",
      "                              'seed': None,\n",
      "                              'sharding': True,\n",
      "                              'shuffle_buffer_size': 10000,\n",
      "                              'tf_data_service_address': None,\n",
      "                              'tf_data_service_job_name': None,\n",
      "                              'tfds_as_supervised': False,\n",
      "                              'tfds_data_dir': '',\n",
      "                              'tfds_name': '',\n",
      "                              'tfds_skip_decoding_feature': '',\n",
      "                              'tfds_split': ''}},\n",
      " 'trainer': {'allow_tpu_summary': False,\n",
      "             'best_checkpoint_eval_metric': '',\n",
      "             'best_checkpoint_export_subdir': '',\n",
      "             'best_checkpoint_metric_comp': 'higher',\n",
      "             'checkpoint_interval': 360,\n",
      "             'continuous_eval_timeout': 3600,\n",
      "             'eval_tf_function': True,\n",
      "             'eval_tf_while_loop': False,\n",
      "             'loss_upper_bound': 1000000.0,\n",
      "             'max_to_keep': 5,\n",
      "             'optimizer_config': {'ema': None,\n",
      "                                  'learning_rate': {'stepwise': {'boundaries': [675,\n",
      "                                                                                1350,\n",
      "                                                                                2025,\n",
      "                                                                                2700,\n",
      "                                                                                3375],\n",
      "                                                                 'name': 'PiecewiseConstantDecay',\n",
      "                                                                 'offset': 0,\n",
      "                                                                 'values': [0.016,\n",
      "                                                                            0.008,\n",
      "                                                                            0.004,\n",
      "                                                                            0.002,\n",
      "                                                                            0.001,\n",
      "                                                                            0.0005]},\n",
      "                                                    'type': 'stepwise'},\n",
      "                                  'optimizer': {'sgd': {'clipnorm': None,\n",
      "                                                        'clipvalue': None,\n",
      "                                                        'decay': 0.0,\n",
      "                                                        'global_clipnorm': None,\n",
      "                                                        'momentum': 0.9,\n",
      "                                                        'name': 'SGD',\n",
      "                                                        'nesterov': False},\n",
      "                                                'type': 'sgd'},\n",
      "                                  'warmup': {'type': None}},\n",
      "             'recovery_begin_steps': 0,\n",
      "             'recovery_max_trials': 0,\n",
      "             'steps_per_loop': 45,\n",
      "             'summary_interval': 45,\n",
      "             'train_steps': 3600,\n",
      "             'train_tf_function': True,\n",
      "             'train_tf_while_loop': True,\n",
      "             'validation_interval': 360,\n",
      "             'validation_steps': 14,\n",
      "             'validation_summary_subdir': 'validation'}}\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [384, 384]\n",
    "\n",
    "# default parameters can be overriden in two ways:\n",
    "# 1) params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
    "# 2) params.task.model.backbone = backbone_cfg\n",
    "# params.override checks that the dictionary keys exist\n",
    "# the second options will silently add new keys\n",
    "\n",
    "params = model_garden.core.exp_factory.get_exp_config('retinanet')\n",
    "\n",
    "params.task.model.num_classes = len(CLASSES)+1 # class 0 is reserved for backgrounds\n",
    "params.task.model.input_size = [*IMAGE_SIZE, 3] # this automatically configures the input reader to random crop training images\n",
    "params.task.init_checkpoint = SPINET_MOBILE_CHECKPOINT\n",
    "params.task.init_checkpoint_modules = 'backbone'\n",
    "params.task.model.backbone = backbones_cfg.Backbone(type='spinenet_mobile', spinenet_mobile=backbones_cfg.SpineNetMobile())\n",
    "\n",
    "train_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=TRAIN_DATA_PATH_PATTERN,\n",
    "    is_training=True,\n",
    "    global_batch_size=BATCH_SIZE,\n",
    "    parser=retinanet_cfg.Parser(aug_rand_hflip=True, aug_scale_min=0.7, aug_scale_max=2.0))\n",
    "\n",
    "valid_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=VALID_DATA_PATH_PATTERN,\n",
    "    is_training=False,\n",
    "    global_batch_size=BATCH_SIZE)\n",
    "\n",
    "params.override({'task': {'train_data': train_data_cfg.as_dict(), 'validation_data': valid_data_cfg.as_dict()}})\n",
    "\n",
    "trainer_cfg=model_garden.core.config_definitions.TrainerConfig(\n",
    "    train_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    validation_steps=VALID_STEPS,\n",
    "    validation_interval=8*STEPS_PER_EPOCH,\n",
    "    steps_per_loop=STEPS_PER_EPOCH,\n",
    "    summary_interval=STEPS_PER_EPOCH,\n",
    "    checkpoint_interval=8*STEPS_PER_EPOCH)\n",
    "\n",
    "optim_cfg = model_garden.modeling.optimization.OptimizationConfig({\n",
    "    'optimizer': {\n",
    "                  'type': 'sgd',\n",
    "                  'sgd': {'momentum': 0.9}},\n",
    "    'learning_rate': {'type': 'stepwise',\n",
    "                      'stepwise': {'boundaries': [15 * STEPS_PER_EPOCH,\n",
    "                                                  30 * STEPS_PER_EPOCH,\n",
    "                                                  45 * STEPS_PER_EPOCH,\n",
    "                                                  60 * STEPS_PER_EPOCH,\n",
    "                                                  75 * STEPS_PER_EPOCH],\n",
    "                                   'values': [0.016, #0.01,\n",
    "                                              0.008, #0.005,\n",
    "                                              0.004, #0.0025,\n",
    "                                              0.002, #0.001,\n",
    "                                              0.001, #0.0005,\n",
    "                                              0.0005]} #0.00025]}\n",
    "                     },\n",
    "    #'warmup': {'type': 'linear','linear': {'warmup_steps': 5*STEPS_PER_EPOCH, 'warmup_learning_rate': 0.00001}}\n",
    "})\n",
    "\n",
    "trainer_cfg.override({'optimizer_config': optim_cfg})\n",
    "params.override({'trainer': trainer_cfg})\n",
    "\n",
    "pp.pprint(params.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = model_garden.core.task_factory.get_task(params.task, logging_dir=MODEL_DIR)\n",
    "\n",
    "# this works too:\n",
    "#task = official.vision.beta.tasks.retinanet.RetinaNetTask(params.task)\n",
    "\n",
    "# this returns a RetinaNetModel\n",
    "#task.build_model()\n",
    "# note: none of the expected model functionalities work: model.fit(), model.predict(), model.save()\n",
    "\n",
    "# this returns the training dataset\n",
    "#train_dataset = task.build_inputs(train_data_cfg)\n",
    "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
    "\n",
    "# this returns the validation dataset\n",
    "#valid_dataset = task.build_inputs(valid_data_cfg)\n",
    "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
    "\n",
    "# this code allows you to see if the TFRecord fields are read correctly\n",
    "#ds = tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN))\n",
    "#dec = official.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
    "#ds = ds.map(dec.decode)\n",
    "\n",
    "# training and validatoin data parsing happens in:\n",
    "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_train_data\n",
    "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_eval_data\n",
    "# official.vision.beta.dataloaders.Parser.parse() # dispatches between _parse_train_data and _parse_eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの学習\n",
    "学習には、TPUv3-8で約30分、ColabのTPUv2-8で40分かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://takumi-misc/arthropod_jobs/1651997039\n",
      "restoring or initializing model...\n",
      "initialized model.\n",
      "train | step:      0 | training until step 3600...\n",
      "train | step:     45 | steps/sec:    0.1 | output: \n",
      "    {'box_loss': 0.008858799,\n",
      "     'cls_loss': 0.9120323,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 1.3549725,\n",
      "     'total_loss': 1.3549725,\n",
      "     'training_loss': 1.3549725}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-45.\n",
      "train | step:     90 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.005126689,\n",
      "     'cls_loss': 0.62790865,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.8842432,\n",
      "     'total_loss': 0.8842432,\n",
      "     'training_loss': 0.8842432}\n",
      "train | step:    495 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0029325197,\n",
      "     'cls_loss': 0.39313075,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5397567,\n",
      "     'total_loss': 0.5397567,\n",
      "     'training_loss': 0.5397567}\n",
      "train | step:    540 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.002894357,\n",
      "     'cls_loss': 0.38299152,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.52770936,\n",
      "     'total_loss': 0.52770936,\n",
      "     'training_loss': 0.52770936}\n",
      "train | step:    585 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0027769457,\n",
      "     'cls_loss': 0.37212503,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5109723,\n",
      "     'total_loss': 0.5109723,\n",
      "     'training_loss': 0.5109723}\n",
      "train | step:    630 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0027462107,\n",
      "     'cls_loss': 0.36185956,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.49916998,\n",
      "     'total_loss': 0.49916998,\n",
      "     'training_loss': 0.49916998}\n",
      "train | step:    675 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0027424318,\n",
      "     'cls_loss': 0.3543576,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.4914792,\n",
      "     'total_loss': 0.4914792,\n",
      "     'training_loss': 0.4914792}\n",
      "train | step:    720 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0026014983,\n",
      "     'cls_loss': 0.34018558,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4702605,\n",
      "     'total_loss': 0.4702605,\n",
      "     'training_loss': 0.4702605}\n",
      "train | step:    765 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0025891608,\n",
      "     'cls_loss': 0.3347459,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.46420386,\n",
      "     'total_loss': 0.46420386,\n",
      "     'training_loss': 0.46420386}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-765.\n",
      "train | step:    810 | steps/sec:    2.1 | output: \n",
      "    {'box_loss': 0.0025039976,\n",
      "     'cls_loss': 0.32784125,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.45304117,\n",
      "     'total_loss': 0.45304117,\n",
      "     'training_loss': 0.45304117}\n",
      "train | step:    855 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0024400663,\n",
      "     'cls_loss': 0.32069814,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44270152,\n",
      "     'total_loss': 0.44270152,\n",
      "     'training_loss': 0.44270152}\n",
      "train | step:    900 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0025181477,\n",
      "     'cls_loss': 0.32036757,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.446275,\n",
      "     'total_loss': 0.446275,\n",
      "     'training_loss': 0.446275}\n",
      "train | step:    945 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0024378335,\n",
      "     'cls_loss': 0.31581813,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43770972,\n",
      "     'total_loss': 0.43770972,\n",
      "     'training_loss': 0.43770972}\n",
      "train | step:    990 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.002443438,\n",
      "     'cls_loss': 0.31387737,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43604928,\n",
      "     'total_loss': 0.43604928,\n",
      "     'training_loss': 0.43604928}\n",
      "train | step:   1035 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0023645684,\n",
      "     'cls_loss': 0.30490518,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42313358,\n",
      "     'total_loss': 0.42313358,\n",
      "     'training_loss': 0.42313358}\n",
      "train | step:   1080 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0024180799,\n",
      "     'cls_loss': 0.3092478,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43015182,\n",
      "     'total_loss': 0.43015182,\n",
      "     'training_loss': 0.43015182}\n",
      "train | step:   1125 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0023923344,\n",
      "     'cls_loss': 0.30166784,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42128453,\n",
      "     'total_loss': 0.42128453,\n",
      "     'training_loss': 0.42128453}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-1125.\n",
      "train | step:   1170 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0023509725,\n",
      "     'cls_loss': 0.29646716,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4140158,\n",
      "     'total_loss': 0.4140158,\n",
      "     'training_loss': 0.4140158}\n",
      "train | step:   1215 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0023250908,\n",
      "     'cls_loss': 0.29285783,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.40911245,\n",
      "     'total_loss': 0.40911245,\n",
      "     'training_loss': 0.40911245}\n",
      "train | step:   1260 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0023297444,\n",
      "     'cls_loss': 0.2932987,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4097859,\n",
      "     'total_loss': 0.4097859,\n",
      "     'training_loss': 0.4097859}\n",
      "train | step:   1305 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.002307435,\n",
      "     'cls_loss': 0.29043138,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.40580317,\n",
      "     'total_loss': 0.40580317,\n",
      "     'training_loss': 0.40580317}\n",
      "train | step:   1350 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0023147457,\n",
      "     'cls_loss': 0.28735685,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4030941,\n",
      "     'total_loss': 0.4030941,\n",
      "     'training_loss': 0.4030941}\n",
      "train | step:   1395 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0022575157,\n",
      "     'cls_loss': 0.28205082,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3949267,\n",
      "     'total_loss': 0.3949267,\n",
      "     'training_loss': 0.3949267}\n",
      "train | step:   1440 | steps/sec:    2.2 | output: \n",
      "    {'box_loss': 0.0022521783,\n",
      "     'cls_loss': 0.27743593,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.39004478,\n",
      "     'total_loss': 0.39004478,\n",
      "     'training_loss': 0.39004478}\n",
      "train | step:   1485 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.002173862,\n",
      "     'cls_loss': 0.2733674,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38206047,\n",
      "     'total_loss': 0.38206047,\n",
      "     'training_loss': 0.38206047}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-1485.\n",
      "train | step:   1530 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.0021865736,\n",
      "     'cls_loss': 0.27224937,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38157812,\n",
      "     'total_loss': 0.38157812,\n",
      "     'training_loss': 0.38157812}\n",
      "train | step:   1575 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.002211478,\n",
      "     'cls_loss': 0.27058816,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3811621,\n",
      "     'total_loss': 0.3811621,\n",
      "     'training_loss': 0.3811621}\n",
      "train | step:   1620 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0022274088,\n",
      "     'cls_loss': 0.2733472,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38471764,\n",
      "     'total_loss': 0.38471764,\n",
      "     'training_loss': 0.38471764}\n",
      "train | step:   1665 | steps/sec:    2.8 | output: \n",
      "    {'box_loss': 0.0021568085,\n",
      "     'cls_loss': 0.26794901,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37578946,\n",
      "     'total_loss': 0.37578946,\n",
      "     'training_loss': 0.37578946}\n",
      "train | step:   1710 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.002167596,\n",
      "     'cls_loss': 0.26644012,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37481987,\n",
      "     'total_loss': 0.37481987,\n",
      "     'training_loss': 0.37481987}\n",
      "train | step:   1755 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0021192236,\n",
      "     'cls_loss': 0.2634434,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3694046,\n",
      "     'total_loss': 0.3694046,\n",
      "     'training_loss': 0.3694046}\n",
      "train | step:   1800 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0021972458,\n",
      "     'cls_loss': 0.2672,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37706232,\n",
      "     'total_loss': 0.37706232,\n",
      "     'training_loss': 0.37706232}\n",
      "train | step:   1845 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0021275317,\n",
      "     'cls_loss': 0.26034713,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3667237,\n",
      "     'total_loss': 0.3667237,\n",
      "     'training_loss': 0.3667237}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-1845.\n",
      "train | step:   1890 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0021769418,\n",
      "     'cls_loss': 0.26536402,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37421107,\n",
      "     'total_loss': 0.37421107,\n",
      "     'training_loss': 0.37421107}\n",
      "train | step:   1935 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.002188536,\n",
      "     'cls_loss': 0.26303557,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3724623,\n",
      "     'total_loss': 0.3724623,\n",
      "     'training_loss': 0.3724623}\n",
      "train | step:   1980 | steps/sec:    3.2 | output: \n",
      "    {'box_loss': 0.0020929764,\n",
      "     'cls_loss': 0.25835964,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3630084,\n",
      "     'total_loss': 0.3630084,\n",
      "     'training_loss': 0.3630084}\n",
      "train | step:   2025 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0020895125,\n",
      "     'cls_loss': 0.25516105,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.35963672,\n",
      "     'total_loss': 0.35963672,\n",
      "     'training_loss': 0.35963672}\n",
      "train | step:   2070 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0021366521,\n",
      "     'cls_loss': 0.25766516,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36449775,\n",
      "     'total_loss': 0.36449775,\n",
      "     'training_loss': 0.36449775}\n",
      "train | step:   2115 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0021214848,\n",
      "     'cls_loss': 0.25610402,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3621783,\n",
      "     'total_loss': 0.3621783,\n",
      "     'training_loss': 0.3621783}\n",
      "train | step:   2160 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0020651703,\n",
      "     'cls_loss': 0.2518124,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3550709,\n",
      "     'total_loss': 0.3550709,\n",
      "     'training_loss': 0.3550709}\n",
      "train | step:   2205 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.002078005,\n",
      "     'cls_loss': 0.25206092,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35596123,\n",
      "     'total_loss': 0.35596123,\n",
      "     'training_loss': 0.35596123}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-2205.\n",
      "train | step:   2250 | steps/sec:    0.9 | output: \n",
      "    {'box_loss': 0.0020777965,\n",
      "     'cls_loss': 0.25145814,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35534793,\n",
      "     'total_loss': 0.35534793,\n",
      "     'training_loss': 0.35534793}\n",
      "train | step:   2295 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0020815453,\n",
      "     'cls_loss': 0.25167435,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35575163,\n",
      "     'total_loss': 0.35575163,\n",
      "     'training_loss': 0.35575163}\n",
      "train | step:   2340 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0020669987,\n",
      "     'cls_loss': 0.24847388,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35182384,\n",
      "     'total_loss': 0.35182384,\n",
      "     'training_loss': 0.35182384}\n",
      "train | step:   2385 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0021243517,\n",
      "     'cls_loss': 0.25303447,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3592521,\n",
      "     'total_loss': 0.3592521,\n",
      "     'training_loss': 0.3592521}\n",
      "train | step:   2430 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0020115129,\n",
      "     'cls_loss': 0.24629675,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34687248,\n",
      "     'total_loss': 0.34687248,\n",
      "     'training_loss': 0.34687248}\n",
      "train | step:   2475 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0020910546,\n",
      "     'cls_loss': 0.24995336,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3545061,\n",
      "     'total_loss': 0.3545061,\n",
      "     'training_loss': 0.3545061}\n",
      "train | step:   2520 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0020749555,\n",
      "     'cls_loss': 0.24866842,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35241625,\n",
      "     'total_loss': 0.35241625,\n",
      "     'training_loss': 0.35241625}\n",
      "train | step:   2565 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.002067955,\n",
      "     'cls_loss': 0.24824026,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35163796,\n",
      "     'total_loss': 0.35163796,\n",
      "     'training_loss': 0.35163796}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-2565.\n",
      "train | step:   2610 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0020700884,\n",
      "     'cls_loss': 0.24840872,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35191306,\n",
      "     'total_loss': 0.35191306,\n",
      "     'training_loss': 0.35191306}\n",
      "train | step:   2655 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0020511819,\n",
      "     'cls_loss': 0.24519236,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3477515,\n",
      "     'total_loss': 0.3477515,\n",
      "     'training_loss': 0.3477515}\n",
      "train | step:   2700 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.002066687,\n",
      "     'cls_loss': 0.24481778,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34815213,\n",
      "     'total_loss': 0.34815213,\n",
      "     'training_loss': 0.34815213}\n",
      "train | step:   2745 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0020311277,\n",
      "     'cls_loss': 0.24228367,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34384003,\n",
      "     'total_loss': 0.34384003,\n",
      "     'training_loss': 0.34384003}\n",
      "train | step:   2790 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0020162005,\n",
      "     'cls_loss': 0.24249545,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34330553,\n",
      "     'total_loss': 0.34330553,\n",
      "     'training_loss': 0.34330553}\n",
      "train | step:   2835 | steps/sec:    2.8 | output: \n",
      "    {'box_loss': 0.002012514,\n",
      "     'cls_loss': 0.23939113,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34001678,\n",
      "     'total_loss': 0.34001678,\n",
      "     'training_loss': 0.34001678}\n",
      "train | step:   2880 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.002073112,\n",
      "     'cls_loss': 0.24630523,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3499608,\n",
      "     'total_loss': 0.3499608,\n",
      "     'training_loss': 0.3499608}\n",
      "train | step:   2925 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0019994634,\n",
      "     'cls_loss': 0.23971036,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33968347,\n",
      "     'total_loss': 0.33968347,\n",
      "     'training_loss': 0.33968347}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-2925.\n",
      "train | step:   2970 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0020158426,\n",
      "     'cls_loss': 0.2411611,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34195322,\n",
      "     'total_loss': 0.34195322,\n",
      "     'training_loss': 0.34195322}\n",
      "train | step:   3015 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0020687457,\n",
      "     'cls_loss': 0.24276742,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34620464,\n",
      "     'total_loss': 0.34620464,\n",
      "     'training_loss': 0.34620464}\n",
      "train | step:   3060 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0019809774,\n",
      "     'cls_loss': 0.23734596,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33639485,\n",
      "     'total_loss': 0.33639485,\n",
      "     'training_loss': 0.33639485}\n",
      "train | step:   3105 | steps/sec:    2.8 | output: \n",
      "    {'box_loss': 0.0020037866,\n",
      "     'cls_loss': 0.24073507,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34092444,\n",
      "     'total_loss': 0.34092444,\n",
      "     'training_loss': 0.34092444}\n",
      "train | step:   3150 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0019722222,\n",
      "     'cls_loss': 0.23842303,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33703417,\n",
      "     'total_loss': 0.33703417,\n",
      "     'training_loss': 0.33703417}\n",
      "train | step:   3195 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.002028543,\n",
      "     'cls_loss': 0.23973289,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34116,\n",
      "     'total_loss': 0.34116,\n",
      "     'training_loss': 0.34116}\n",
      "train | step:   3240 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.002018878,\n",
      "     'cls_loss': 0.23959194,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34053585,\n",
      "     'total_loss': 0.34053585,\n",
      "     'training_loss': 0.34053585}\n",
      "train | step:   3285 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0020459755,\n",
      "     'cls_loss': 0.24021816,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3425169,\n",
      "     'total_loss': 0.3425169,\n",
      "     'training_loss': 0.3425169}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-3285.\n",
      "train | step:   3330 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.002016892,\n",
      "     'cls_loss': 0.23791018,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3387548,\n",
      "     'total_loss': 0.3387548,\n",
      "     'training_loss': 0.3387548}\n",
      "train | step:   3375 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0019612536,\n",
      "     'cls_loss': 0.23795429,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33601686,\n",
      "     'total_loss': 0.33601686,\n",
      "     'training_loss': 0.33601686}\n",
      "train | step:   3420 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0020300767,\n",
      "     'cls_loss': 0.23828723,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33979103,\n",
      "     'total_loss': 0.33979103,\n",
      "     'training_loss': 0.33979103}\n",
      "train | step:   3465 | steps/sec:    2.4 | output: \n",
      "    {'box_loss': 0.0020254257,\n",
      "     'cls_loss': 0.2396777,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34094897,\n",
      "     'total_loss': 0.34094897,\n",
      "     'training_loss': 0.34094897}\n",
      "train | step:   3510 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0020316676,\n",
      "     'cls_loss': 0.2402549,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.34183824,\n",
      "     'total_loss': 0.34183824,\n",
      "     'training_loss': 0.34183824}\n",
      "train | step:   3555 | steps/sec:    2.8 | output: \n",
      "    {'box_loss': 0.002027572,\n",
      "     'cls_loss': 0.2370992,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33847776,\n",
      "     'total_loss': 0.33847776,\n",
      "     'training_loss': 0.33847776}\n",
      "train | step:   3600 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0019672,\n",
      "     'cls_loss': 0.23522644,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.3335865,\n",
      "     'total_loss': 0.3335865,\n",
      "     'training_loss': 0.3335865}\n",
      "saved checkpoint to gs://takumi-misc/arthropod_jobs/1651997039/ckpt-3600.\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIR)\n",
    "model,_ = train_lib.run_experiment(\n",
    "    distribution_strategy=strategy,\n",
    "    task=task,\n",
    "    mode=\"train\", # 'train', 'eval', 'train_and_eval' or 'continuous_eval'\n",
    "    params=params,\n",
    "    model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルのエクスポート\n",
    "エクスポートされたモデルをテストするには、04ac_retinanet_arthropods_predict.ipynbでパスを指定して使用してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 08:36:06.593342: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7efd0637b050>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7efd0637b050>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.merge.Add object at 0x7efd07466350>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.merge.Add object at 0x7efd07466350>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.merge.Add object at 0x7efd0708b990>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.merge.Add object at 0x7efd0708b990>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7efd065c4b10>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7efd065c4b10>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as inference_for_tflite, inference_from_image_bytes, inference_from_tf_example, retina_net_head_1_layer_call_fn, retina_net_head_1_layer_call_and_return_conditional_losses while saving (showing 5 of 345). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://takumi-misc/arthropod_jobs/1651997039/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://takumi-misc/arthropod_jobs/1651997039/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_saved_model_lib.export_inference_graph(\n",
    "      input_type='image_tensor',\n",
    "      batch_size=4,\n",
    "      input_image_size=IMAGE_SIZE,\n",
    "      params=params,\n",
    "      checkpoint_path=MODEL_DIR,\n",
    "      export_dir=MODEL_DIR,\n",
    "      export_checkpoint_subdir='saved_chkpt',\n",
    "      export_saved_model_subdir='saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のパスをコピーし、次のノートブックで使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'saved model path: {MODEL_DIR}/saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
