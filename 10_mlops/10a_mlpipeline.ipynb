{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "hiQ6zAoYhyaA",
    "outputId": "0acee878-1207-42c3-9bee-a594acd44365"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?name=ML+Pipeline&download_url=https%3A%2F%2Fgithub.com%2Ftakumiohym%2Fpractical-ml-vision-book-ja%2Fraw%2Fmaster%2F10_mlops%2F10a_mlpipeline.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/10_mlops/10a_mlpipeline.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/10_mlops/10a_mlpipeline.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmt: off\n",
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"10_mlops/10a_mlpipeline.ipynb\"\n",
    "_nb_title = \"ML Pipeline\"\n",
    "\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/takumiohym/practical-ml-vision-book-ja/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/>\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3]))\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**このノートブックはColabでは実行できません。Vertex AI Workbanchを利用してください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# 機械学習パイプライン  \n",
    "\n",
    "このノートブックでは、花の分類モデルを作成するための一連のワークフローをパイプラインとして実行する方法を示します\n",
    "\n",
    "**このノートブックは、さまざまなGoogle Cloudサービスを実行します。各サービスの料金がかかることに注意してください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user kfp google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # Change as needed to a region where you have quota\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT = PROJECT[0]\n",
    "print(PROJECT)\n",
    "%env PROJECT = {PROJECT}\n",
    "%env REGION = {REGION}\n",
    "BUCKET = PROJECT + \"-flowers-pipeline\"\n",
    "%env BUCKET = {BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l {REGION} gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテナをビルドする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build_docker_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPEGファイルをTF Recordsに変換するコンポーネント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "echo > components/create_dataset.yaml \"name: create_dataset\n",
    "description: Converts JPEG files to TensorFlow Records using Dataflow or Apache Beam\n",
    "inputs:\n",
    "- {name: runner, type: String, default: 'DataflowRunner', description: 'DirectRunner or DataflowRunner'}\n",
    "- {name: project_id, type: String, description: 'Project to bill Dataflow job to'}\n",
    "- {name: region, type: String, description: 'Region to run Dataflow job in'}\n",
    "- {name: input_csv, type: String, description: 'Path to CSV file'}\n",
    "- {name: output_dir, type: String, description: 'Top-level directory for TF records'}\n",
    "- {name: labels_dict, type: String, description: 'Dictionary file for class names'}\n",
    "outputs:\n",
    "- {name: tfrecords_topdir, type: String, description: 'Top-level directory for TF records'}\n",
    "implementation:\n",
    "  container:\n",
    "    image: us-docker.pkg.dev/$PROJECT/practical-ml-vision-book/mlops\n",
    "    command: [\n",
    "        'bash', '/src/practical-ml-vision-book/10_mlops/components/create_dataset.sh'\n",
    "    ]\n",
    "    args: [\n",
    "        {inputValue: output_dir},\n",
    "        {outputPath: tfrecords_topdir},\n",
    "        '--all_data', {inputValue: input_csv},\n",
    "        '--labels_file', {inputValue: labels_dict},\n",
    "        '--project_id', {inputValue: project_id},\n",
    "        '--output_dir', {inputValue: output_dir},\n",
    "        '--runner', {inputValue: runner},\n",
    "        '--region', {inputValue: region},\n",
    "    ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing flowers.egg-info/PKG-INFO\n",
      "writing dependency_links to flowers.egg-info/dependency_links.txt\n",
      "writing requirements to flowers.egg-info/requires.txt\n",
      "writing top-level names to flowers.egg-info/top_level.txt\n",
      "reading manifest file 'flowers.egg-info/SOURCES.txt'\n",
      "writing manifest file 'flowers.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating flowers-1.0\n",
      "creating flowers-1.0/flowers\n",
      "creating flowers-1.0/flowers.egg-info\n",
      "creating flowers-1.0/flowers/classifier\n",
      "creating flowers-1.0/flowers/ingest\n",
      "creating flowers-1.0/flowers/utils\n",
      "copying files to flowers-1.0...\n",
      "copying README.txt -> flowers-1.0\n",
      "copying setup.py -> flowers-1.0\n",
      "copying flowers/__init__.py -> flowers-1.0/flowers\n",
      "copying flowers.egg-info/PKG-INFO -> flowers-1.0/flowers.egg-info\n",
      "copying flowers.egg-info/SOURCES.txt -> flowers-1.0/flowers.egg-info\n",
      "copying flowers.egg-info/dependency_links.txt -> flowers-1.0/flowers.egg-info\n",
      "copying flowers.egg-info/requires.txt -> flowers-1.0/flowers.egg-info\n",
      "copying flowers.egg-info/top_level.txt -> flowers-1.0/flowers.egg-info\n",
      "copying flowers/classifier/__init__.py -> flowers-1.0/flowers/classifier\n",
      "copying flowers/classifier/model.py -> flowers-1.0/flowers/classifier\n",
      "copying flowers/classifier/train.py -> flowers-1.0/flowers/classifier\n",
      "copying flowers/ingest/__init__.py -> flowers-1.0/flowers/ingest\n",
      "copying flowers/ingest/tfrecords.py -> flowers-1.0/flowers/ingest\n",
      "copying flowers/utils/__init__.py -> flowers-1.0/flowers/utils\n",
      "copying flowers/utils/augment.py -> flowers-1.0/flowers/utils\n",
      "copying flowers/utils/plots.py -> flowers-1.0/flowers/utils\n",
      "copying flowers/utils/util.py -> flowers-1.0/flowers/utils\n",
      "Writing flowers-1.0/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'flowers-1.0' (and everything under it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing required meta-data: url\n",
      "\n",
      "Copying file://./dist/flowers-1.0.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  6.4 KiB/  6.4 KiB]                                                \n",
      "Operation completed over 1 objects/6.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../07_training/serverlessml\n",
    "python ./setup.py sdist --formats=gztar\n",
    "gsutil cp ./dist/flowers-1.0.tar.gz gs://${BUCKET}/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パイプライン定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.v2.dsl as dsl\n",
    "from kfp.v2.dsl import component\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.aiplatform import (CustomPythonPackageTrainingJobRunOp,\n",
    "                                                         ModelUploadOp,\n",
    "                                                         EndpointCreateOp,\n",
    "                                                         ModelDeployOp)\n",
    "\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "create_dataset_op = kfp.components.load_component_from_file(\n",
    "    'components/create_dataset.yaml'\n",
    ")\n",
    "\n",
    "@component(base_image=\"python:3.8\")\n",
    "def construct_workerpool_spec_op(\n",
    "    container_uri:str,\n",
    "    machine_type:str,\n",
    "    replica_count:int,\n",
    "    accelerator_type:str,\n",
    "    accelerator_count:int,\n",
    "    bucket:str,\n",
    "    timestamp:str,\n",
    "    input_top_dir:str,\n",
    "    num_epochs:int,\n",
    "    distribute:str,\n",
    "    pattern:str\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [(\"workerpoolspec\", list)]\n",
    "):\n",
    "    args = [f'--job_dir=gs://{bucket}/trained_model/{timestamp}',\n",
    "            f'--input_topdir={input_top_dir.strip()}',\n",
    "            f'--pattern={pattern}',\n",
    "            f'--num_epochs={num_epochs}',\n",
    "            f'--distribute={distribute}']\n",
    "\n",
    "    worker_pool_specs=[\n",
    "        {\n",
    "            \"pythonPackageSpec\": {\n",
    "                \"args\": args,\n",
    "                \"executorImageUri\": container_uri,\n",
    "                \"packageUris\": [f\"gs://{bucket}/model/flowers-1.0.tar.gz\"],\n",
    "                \"pythonModule\": \"flowers.classifier.train\"\n",
    "            },\n",
    "            \"replicaCount\": replica_count,\n",
    "            \"machineSpec\": {\n",
    "                \"machineType\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('Outputs', ['workerpoolspec'])\n",
    "    \n",
    "    return output(worker_pool_specs)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='flowers-transfer-learning-pipeline',\n",
    "    description='End-to-end pipeline',\n",
    "    pipeline_root=f'gs://{os.getenv(\"BUCKET\")}/pipeline',\n",
    ")\n",
    "def flowerstxf_pipeline(\n",
    "    project_id:str,\n",
    "    bucket:str,\n",
    "    region:str,\n",
    "    timestamp:str\n",
    "):\n",
    "\n",
    "    # Create dataset\n",
    "    create_dataset = create_dataset_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        input_csv='gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/all_data.csv',\n",
    "        output_dir=f'gs://{bucket}/data/flower_tfrecords/{timestamp}',\n",
    "        labels_dict='gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dict.txt'\n",
    "    )\n",
    "\n",
    "    construct_workerpool_spec = construct_workerpool_spec_op(\n",
    "        container_uri='us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest',\n",
    "        machine_type='n1-highmem-8',\n",
    "        replica_count=1,\n",
    "        accelerator_type='NVIDIA_TESLA_T4',\n",
    "        accelerator_count=2,\n",
    "        bucket=bucket,\n",
    "        timestamp=timestamp,\n",
    "        input_top_dir=create_dataset.outputs['tfrecords_topdir'],\n",
    "        num_epochs=20,\n",
    "        distribute='gpus_one_machine',\n",
    "        pattern='-*'\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_model = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name=f'flowers_{timestamp}_gpus_one_machine',\n",
    "        worker_pool_specs=construct_workerpool_spec.outputs['workerpoolspec']\n",
    "    )\n",
    "\n",
    "    # Deploy trained model\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        display_name=f\"flower-model-{timestamp}\",\n",
    "        project=project_id,\n",
    "        artifact_uri=f\"gs://{bucket}/trained_model/{timestamp}/flowers_model\",\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-8:latest\",\n",
    "    ).after(train_model)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=f\"flower-endpoint-{timestamp}\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=f'flower_model_{timestamp}',\n",
    "        dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パイプラインのコンパイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_JSON = \"flowerstxf_pipeline.json\"\n",
    "\n",
    "_compiler = kfp.v2.compiler.Compiler()\n",
    "_compiler.compile(pipeline_func=flowerstxf_pipeline, package_path=PIPELINE_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Pipelinesへジョブを送信する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/flowers-transfer-learning-pipeline-20221212000503?project=849204435784\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/849204435784/locations/us-central1/pipelineJobs/flowers-transfer-learning-pipeline-20221212000503\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"flower_transferlearning_pipeline_cloud\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    "    parameter_values={'project_id': PROJECT, 'bucket': BUCKET, 'region': REGION, 'timestamp': TIMESTAMP},\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu8mX3iXANE"
   },
   "source": [
    "## License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "10a_mlpipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
