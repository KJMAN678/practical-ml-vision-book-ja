{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "hiQ6zAoYhyaA",
    "outputId": "0acee878-1207-42c3-9bee-a594acd44365"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"10_mlops/10a_mlpipeline.ipynb\"\n",
    "_nb_title = \"ML Pipeline\"\n",
    "\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/takumiohym/practical-ml-vision-book-ja/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/>\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# 機械学習パイプライン  \n",
    "\n",
    "このノートブックでは、花の分類モデルを作成するための一連のワークフローをパイプラインとして実行する方法を示します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user kfp google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # Change as needed to a region where you have quota\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT = PROJECT[0]\n",
    "print(PROJECT)\n",
    "%env PROJECT = {PROJECT}\n",
    "%env REGION = {REGION}\n",
    "BUCKET = PROJECT + \"-flowers-pipeline\"\n",
    "%env BUCKET = {BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -l {REGION} gs://{BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンテナをビルドする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build_docker_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPEGファイルをTF Recordsに変換するコンポーネント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "echo > components/create_dataset.yaml \"name: create_dataset\n",
    "description: Converts JPEG files to TensorFlow Records using Dataflow or Apache Beam\n",
    "inputs:\n",
    "- {name: runner, type: String, default: 'DataflowRunner', description: 'DirectRunner or DataflowRunner'}\n",
    "- {name: project_id, type: String, description: 'Project to bill Dataflow job to'}\n",
    "- {name: region, type: String, description: 'Region to run Dataflow job in'}\n",
    "- {name: input_csv, type: String, description: 'Path to CSV file'}\n",
    "- {name: output_dir, type: String, description: 'Top-level directory for TF records'}\n",
    "- {name: labels_dict, type: String, description: 'Dictionary file for class names'}\n",
    "outputs:\n",
    "- {name: tfrecords_topdir, type: String, description: 'Top-level directory for TF records'}\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/$PROJECT/practical-ml-vision-book:latest\n",
    "    command: [\n",
    "        'bash', '/src/practical-ml-vision-book/10_mlops/components/create_dataset.sh'\n",
    "    ]\n",
    "    args: [\n",
    "        {inputValue: output_dir},\n",
    "        {outputPath: tfrecords_topdir},\n",
    "        '--all_data', {inputValue: input_csv},\n",
    "        '--labels_file', {inputValue: labels_dict},\n",
    "        '--project_id', {inputValue: project_id},\n",
    "        '--output_dir', {inputValue: output_dir},\n",
    "        '--runner', {inputValue: runner},\n",
    "        '--region', {inputValue: region},\n",
    "    ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../07_training/serverlessml\n",
    "python ./setup.py sdist --formats=gztar\n",
    "gsutil cp ./dist/flowers-1.0.tar.gz gs://${BUCKET}/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パイプライン定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.v2.dsl as dsl\n",
    "from kfp.v2.dsl import component\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.aiplatform import (CustomPythonPackageTrainingJobRunOp,\n",
    "                                                         ModelUploadOp,\n",
    "                                                         EndpointCreateOp,\n",
    "                                                         ModelDeployOp)\n",
    "\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "create_dataset_op = kfp.components.load_component_from_file(\n",
    "    'components/create_dataset.yaml'\n",
    ")\n",
    "\n",
    "@component(base_image=\"python:3.8\")\n",
    "def construct_workerpool_spec_op(\n",
    "    container_uri:str,\n",
    "    machine_type:str,\n",
    "    replica_count:int,\n",
    "    accelerator_type:str,\n",
    "    accelerator_count:int,\n",
    "    bucket:str,\n",
    "    timestamp:str,\n",
    "    input_top_dir:str,\n",
    "    num_epochs:int,\n",
    "    distribute:str,\n",
    "    pattern:str\n",
    ")-> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [(\"workerpoolspec\", list)]\n",
    "):\n",
    "    args = [f'--job_dir=gs://{bucket}/trained_model/{timestamp}',\n",
    "            f'--input_topdir={input_top_dir.strip()}',\n",
    "            f'--pattern={pattern}',\n",
    "            f'--num_epochs={num_epochs}',\n",
    "            f'--distribute={distribute}']\n",
    "\n",
    "    worker_pool_specs=[\n",
    "        {\n",
    "            \"pythonPackageSpec\": {\n",
    "                \"args\": args,\n",
    "                \"executorImageUri\": container_uri,\n",
    "                \"packageUris\": [f\"gs://{bucket}/model/flowers-1.0.tar.gz\"],\n",
    "                \"pythonModule\": \"flowers.classifier.train\"\n",
    "            },\n",
    "            \"replicaCount\": replica_count,\n",
    "            \"machineSpec\": {\n",
    "                \"machineType\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('Outputs', ['workerpoolspec'])\n",
    "    \n",
    "    return output(worker_pool_specs)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='flowers-transfer-learning-pipeline',\n",
    "    description='End-to-end pipeline',\n",
    "    pipeline_root=f'gs://{os.getenv(\"BUCKET\")}/pipeline',\n",
    ")\n",
    "def flowerstxf_pipeline(\n",
    "    project_id:str,\n",
    "    bucket:str,\n",
    "    region:str,\n",
    "    timestamp:str\n",
    "):\n",
    "\n",
    "    # Create dataset\n",
    "    create_dataset = create_dataset_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        input_csv='gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/all_data.csv',\n",
    "        output_dir=f'gs://{bucket}/data/flower_tfrecords/{timestamp}',\n",
    "        labels_dict='gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dict.txt'\n",
    "    )\n",
    "\n",
    "    construct_workerpool_spec = construct_workerpool_spec_op(\n",
    "        container_uri='us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest',\n",
    "        machine_type='n1-highmem-8',\n",
    "        replica_count=1,\n",
    "        accelerator_type='NVIDIA_TESLA_T4',\n",
    "        accelerator_count=2,\n",
    "        bucket=bucket,\n",
    "        timestamp=timestamp,\n",
    "        input_top_dir=create_dataset.outputs['tfrecords_topdir'],\n",
    "        num_epochs=20,\n",
    "        distribute='gpus_one_machine',\n",
    "        pattern='-*'\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_model = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name=f'flowers_{timestamp}_gpus_one_machine',\n",
    "        worker_pool_specs=construct_workerpool_spec.outputs['workerpoolspec']\n",
    "    )\n",
    "\n",
    "    # Deploy trained model\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        display_name=f\"flower-model-{timestamp}\",\n",
    "        project=project_id,\n",
    "        artifact_uri=f\"gs://{bucket}/trained_model/{timestamp}/flowers_model\",\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-8:latest\",\n",
    "    ).after(train_model)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=f\"flower-endpoint-{timestamp}\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=f'flower_model_{timestamp}',\n",
    "        dedicated_resources_machine_type=\"n1-standard-16\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パイプラインのコンパイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"flowerstxf_pipeline.json\"\n",
    "\n",
    "_compiler = kfp.v2.compiler.Compiler()\n",
    "_compiler.compile(pipeline_func=flowerstxf_pipeline, package_path=PIPELINE_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Pipelinesへジョブを送信する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"flower_transferlearning_pipeline_cloud\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    "    parameter_values={'project_id': PROJECT, 'bucket': BUCKET, 'region': REGION, 'timestamp': TIMESTAMP},\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu8mX3iXANE"
   },
   "source": [
    "## License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "10a_mlpipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
