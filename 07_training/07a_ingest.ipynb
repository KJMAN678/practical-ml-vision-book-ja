{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 72}, "id": "hiQ6zAoYhyaA", "outputId": "0acee878-1207-42c3-9bee-a594acd44365"}, "outputs": [{"data": {"text/markdown": ["\n", "<table class=\"tfo-notebook-buttons\" align=\"left\">\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Writing an efficient ingest Loop&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fblob%2Fmaster%2F06_preprocessing%2F07a_ingest.ipynb&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F06_preprocessing%2F07a_ingest.ipynb\">\n", "    <img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/> Run in AI Platform Notebook</a>\n", "  </td>\n", "  </td>\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/06_preprocessing/07a_ingest.ipynb\">\n", "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n", "  </td>\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/06_preprocessing/07a_ingest.ipynb\">\n", "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n", "  </td>\n", "  <td>\n", "    <a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/06_preprocessing/07a_ingest.ipynb\">\n", "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n", "  </td>\n", "</table>\n"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["from IPython.display import Markdown as md\n", "\n", "### change to reflect your notebook\n", "_nb_loc = \"07_training/07a_ingest.ipynb\"\n", "_nb_title = \"Writing an efficient ingest Loop\"\n", "\n", "### no need to change any of this\n", "_nb_safeloc = _nb_loc.replace('/', '%2F')\n", "md(\"\"\"\n", "<table class=\"tfo-notebook-buttons\" align=\"left\">\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name={1}&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fblob%2Fmaster%2F{2}&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F{2}\">\n", "    <img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/> Run in AI Platform Notebook</a>\n", "  </td>\n", "  </td>\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\">\n", "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n", "  </td>\n", "  <td>\n", "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\">\n", "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n", "  </td>\n", "  <td>\n", "    <a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/{0}\">\n", "    <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n", "  </td>\n", "</table>\n", "\"\"\".format(_nb_loc, _nb_title, _nb_safeloc))"]}, {"cell_type": "markdown", "metadata": {"id": "a8HQYsAtC0Fv"}, "source": ["# \u52b9\u7387\u7684\u306a\u6442\u53d6  \n", "\n", "\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0/\u8a55\u4fa1\u30c7\u30fc\u30bf\u306e\u30e2\u30c7\u30eb\u3078\u306e\u53d6\u308a\u8fbc\u307f\u3092\u9ad8\u901f\u5316\u3057\u307e\u3059\u3002"]}, {"cell_type": "markdown", "metadata": {"id": "5UOm2etrwYCs"}, "source": ["## GPU\u3092\u6709\u52b9\u306b\u3057\u3001\u30d8\u30eb\u30d1\u30fc\u95a2\u6570\u3092\u8a2d\u5b9a\u3057\u307e\u3059  \n", "\n", "\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3068\u3001\u3053\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u5185\u306e\u4ed6\u306e\u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af  \n", "GPU\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u3088\u308a\u9ad8\u901f\u306b\u5b9f\u884c\u3055\u308c\u307e\u3059\u3002  \n", "Colab\u306b\u3064\u3044\u3066\uff1a  \n", "- [\u7de8\u96c6]\u2192[\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u8a2d\u5b9a]\u306b\u79fb\u52d5\u3057\u307e\u3059  \n", "- [\u30cf\u30fc\u30c9\u30a6\u30a7\u30a2\u30a2\u30af\u30bb\u30e9\u30ec\u30fc\u30bf]\u30c9\u30ed\u30c3\u30d7\u30c0\u30a6\u30f3\u304b\u3089[GPU]\u3092\u9078\u629e\u3057\u307e\u3059  \n", "\n", "\u30af\u30e9\u30a6\u30c9AI\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\uff1a  \n", "- https://console.cloud.google.com/ai-platform/notebooks\u306b\u79fb\u52d5\u3057\u307e\u3059  \n", "- GPU\u3092\u4f7f\u7528\u3057\u3066\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\u3059\u308b\u304b\u3001\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u9078\u629e\u3057\u3066GPU\u3092\u8ffd\u52a0\u3057\u307e\u3059  \n", "\n", "\u6b21\u306b\u3001\u30c6\u30f3\u30bd\u30eb\u30d5\u30ed\u30fc\u3092\u4f7f\u7528\u3057\u3066GPU\u306b\u63a5\u7d9a\u3067\u304d\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ugGJcxKAwhc2", "outputId": "8e946159-46cf-4aba-f53e-622e9ea8adee"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["TensorFlow version2.3.1\n", "Built with GPU support? Yes!\n", "There are 2 GPUs\n", "Found GPU at: /device:GPU:0\n"]}], "source": ["import tensorflow as tf\n", "print('TensorFlow version' + tf.version.VERSION)\n", "print('Built with GPU support? ' + ('Yes!' if tf.test.is_built_with_cuda() else 'Noooo!'))\n", "print('There are {} GPUs'.format(len(tf.config.experimental.list_physical_devices(\"GPU\"))))\n", "device_name = tf.test.gpu_device_name()\n", "if device_name != '/device:GPU:0':\n", "  raise SystemError('GPU device not found')\n", "print('Found GPU at: {}'.format(device_name))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u5143\u306e\u30b3\u30fc\u30c9  \n", "\n", "\u3053\u308c\u306f\u3001[../06_preprocessing/06e_colorConstraintion.ipynb](../06_preprocessing/06e_colorConstraintion.ipynb)\u306e\u5143\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002  \n", "\n", "\u524d\u51e6\u7406\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u30d0\u30ea\u30a8\u30fc\u30b7\u30e7\u30f3\u304c\u3044\u304f\u3064\u304b\u3042\u308a\u307e\u3059\u3002"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["import matplotlib.pylab as plt\n", "import numpy as np\n", "import tensorflow as tf\n", "import tensorflow_hub as hub\n", "import os\n", "# Load compressed models from tensorflow_hub\n", "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'    \n", "\n", "from tensorflow.data.experimental import AUTOTUNE\n", "\n", "IMG_HEIGHT = 448 # note *twice* what we used to have\n", "IMG_WIDTH = 448\n", "IMG_CHANNELS = 3\n", "CLASS_NAMES = 'daisy dandelion roses sunflowers tulips'.split()\n", "\n", "def training_plot(metrics, history):\n", "  f, ax = plt.subplots(1, len(metrics), figsize=(5*len(metrics), 5))\n", "  for idx, metric in enumerate(metrics):\n", "    ax[idx].plot(history.history[metric], ls='dashed')\n", "    ax[idx].set_xlabel(\"Epochs\")\n", "    ax[idx].set_ylabel(metric)\n", "    ax[idx].plot(history.history['val_' + metric]);\n", "    ax[idx].legend([metric, 'val_' + metric])\n", "    \n", "class _Preprocessor:    \n", "    def __init__(self):\n", "        # nothing to initialize\n", "        pass\n", "    \n", "    def read_from_tfr(self, proto):\n", "        feature_description = {\n", "            'image': tf.io.VarLenFeature(tf.float32),\n", "            'shape': tf.io.VarLenFeature(tf.int64),\n", "            'label': tf.io.FixedLenFeature([], tf.string, default_value=''),\n", "            'label_int': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n", "        }\n", "        rec = tf.io.parse_single_example(\n", "            proto, feature_description\n", "        )\n", "        shape = tf.sparse.to_dense(rec['shape'])\n", "        img = tf.reshape(tf.sparse.to_dense(rec['image']), shape)\n", "        label_int = rec['label_int']\n", "        return img, label_int\n", "    \n", "    def read_from_jpegfile(self, filename):\n", "        # same code as in 05_create_dataset/jpeg_to_tfrecord.py\n", "        img = tf.io.read_file(filename)\n", "        img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n", "        img = tf.image.convert_image_dtype(img, tf.float32)\n", "        return img\n", "      \n", "    def preprocess(self, img):\n", "        return tf.image.resize_with_pad(img, IMG_HEIGHT, IMG_WIDTH)\n", "\n", "def create_preproc_dataset_plain(pattern):\n", "    preproc = _Preprocessor()\n", "    trainds = tf.data.TFRecordDataset(\n", "        [filename for filename in tf.io.gfile.glob(pattern)],\n", "        compression_type='GZIP'\n", "    ).map(preproc.read_from_tfr).map(\n", "        lambda img, label: (preproc.preprocess(img), label)\n", "    )                             \n", "    return trainds\n", "\n", "# note: addition of AUTOTUNE to the map() calls\n", "def create_preproc_dataset_parallelmap(pattern):\n", "    preproc = _Preprocessor()\n", "    def _preproc_img_label(img, label):\n", "        return (preproc.preprocess(img), label)\n", "    trainds = (\n", "        tf.data.TFRecordDataset(\n", "            [filename for filename in tf.io.gfile.glob(pattern)],\n", "            compression_type='GZIP'\n", "        )\n", "        .map(preproc.read_from_tfr, num_parallel_calls=AUTOTUNE)\n", "        .map(_preproc_img_label, num_parallel_calls=AUTOTUNE)\n", "    )\n", "    return trainds\n", "\n", "# note: splits the files into two halves and interleaves datasets\n", "def create_preproc_dataset_interleave(pattern, num_parallel=None):\n", "    preproc = _Preprocessor()\n", "    files = [filename for filename in tf.io.gfile.glob(pattern)]\n", "    if len(files) > 1:\n", "        print(\"Interleaving the reading of {} files.\".format(len(files)))\n", "        def _create_half_ds(x):\n", "            if x == 0:\n", "                half = files[:(len(files)//2)]\n", "            else:\n", "                half = files[(len(files)//2):]\n", "            return tf.data.TFRecordDataset(half,\n", "                                          compression_type='GZIP')\n", "        trainds = tf.data.Dataset.range(2).interleave(\n", "            _create_half_ds, num_parallel_calls=AUTOTUNE)\n", "    else:\n", "        trainds = tf.data.TFRecordDataset(files,\n", "                                         compression_type='GZIP')\n", "    def _preproc_img_label(img, label):\n", "        return (preproc.preprocess(img), label)\n", "    \n", "    trainds = (trainds\n", "               .map(preproc.read_from_tfr, num_parallel_calls=num_parallel)\n", "               .map(_preproc_img_label, num_parallel_calls=num_parallel)\n", "              )\n", "    return trainds\n", "\n", "def create_preproc_image(filename):\n", "    preproc = _Preprocessor()\n", "    img = preproc.read_from_jpegfile(filename)\n", "    return preproc.preprocess(img)\n", "\n", "class RandomColorDistortion(tf.keras.layers.Layer):\n", "    def __init__(self, contrast_range=[0.5, 1.5], \n", "                 brightness_delta=[-0.2, 0.2], **kwargs):\n", "        super(RandomColorDistortion, self).__init__(**kwargs)\n", "        self.contrast_range = contrast_range\n", "        self.brightness_delta = brightness_delta\n", "    \n", "    def call(self, images, training=None):\n", "        if not training:\n", "            return images\n", "        \n", "        contrast = np.random.uniform(\n", "            self.contrast_range[0], self.contrast_range[1])\n", "        brightness = np.random.uniform(\n", "            self.brightness_delta[0], self.brightness_delta[1])\n", "        \n", "        images = tf.image.adjust_contrast(images, contrast)\n", "        images = tf.image.adjust_brightness(images, brightness)\n", "        images = tf.clip_by_value(images, 0, 1)\n", "        return images"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u53d6\u308a\u3092\u9ad8\u901f\u5316  \n", "\n", "\u8a66\u3057\u3066\u307f\u308b\u305f\u3081\u306b\u3001\u30c7\u30fc\u30bf\u3092\u6570\u56de\u8aad\u307f\u53d6\u308a\u3001\u753b\u50cf\u306e\u91cf\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def loop_through_dataset(ds, nepochs):\n", "    lowest_mean = tf.constant(1.)\n", "    for epoch in range(nepochs):\n", "        thresh = np.random.uniform(0.3, 0.7) # random threshold\n", "        count = 0\n", "        sumsofar = tf.constant(0.)\n", "        for (img, label) in ds:\n", "            # mean of channel values > thresh\n", "            mean = tf.reduce_mean(tf.where(img > thresh, img, 0))\n", "            sumsofar = sumsofar + mean\n", "            count = count + 1\n", "            if count%100 == 0:\n", "                print('.', end='')\n", "        mean = sumsofar/count\n", "        print(mean)\n", "        if mean < lowest_mean:\n", "            lowest_mean = mean\n", "    return lowest_mean"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["PATTERN_SUFFIX, NUM_EPOCHS = '-0000[01]-*', 2 # 2 files, 2 epochs\n", "#PATTERN_SUFFIX, NUM_EPOCHS = '-*', 20 # 16 files, 20 epochs"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["...tf.Tensor(0.22762734, shape=(), dtype=float32)\n", "...tf.Tensor(0.21017572, shape=(), dtype=float32)\n", "CPU times: user 7.03 s, sys: 500 ms, total: 7.53 s\n", "Wall time: 7.99 s\n"]}, {"data": {"text/plain": ["<tf.Tensor: shape=(), dtype=float32, numpy=0.21017572>"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["%%time\n", "ds = create_preproc_dataset_plain(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ")\n", "loop_through_dataset(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["...tf.Tensor(0.14244522, shape=(), dtype=float32)\n", "...tf.Tensor(0.18533988, shape=(), dtype=float32)\n", "CPU times: user 7.93 s, sys: 375 ms, total: 8.3 s\n", "Wall time: 5.94 s\n"]}, {"data": {"text/plain": ["<tf.Tensor: shape=(), dtype=float32, numpy=0.14244522>"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["%%time\n", "# parallel map\n", "ds = create_preproc_dataset_parallelmap(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ")\n", "loop_through_dataset(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "first half\n", "second half\n", "...tf.Tensor(0.12316246, shape=(), dtype=float32)\n", "...tf.Tensor(0.15402032, shape=(), dtype=float32)\n", "CPU times: user 7.86 s, sys: 497 ms, total: 8.35 s\n", "Wall time: 5.29 s\n"]}, {"data": {"text/plain": ["<tf.Tensor: shape=(), dtype=float32, numpy=0.12316246>"]}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": ["%%time\n", "# with interleave\n", "ds = create_preproc_dataset_interleave(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n", "    num_parallel=None\n", ")\n", "loop_through_dataset(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "...tf.Tensor(0.18058446, shape=(), dtype=float32)\n", "...tf.Tensor(0.14600855, shape=(), dtype=float32)\n", "CPU times: user 7.99 s, sys: 443 ms, total: 8.44 s\n", "Wall time: 5.23 s\n"]}, {"data": {"text/plain": ["<tf.Tensor: shape=(), dtype=float32, numpy=0.14600855>"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["%%time\n", "# with interleave and parallel mpas\n", "ds = create_preproc_dataset_interleave(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n", "    num_parallel=AUTOTUNE\n", ")\n", "loop_through_dataset(ds, NUM_EPOCHS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u79c1\u304c\u3053\u308c\u3092\u3057\u305f\u3068\u304d\u3001\u3053\u308c\u306f\u79c1\u304c\u5f97\u305f\u3082\u306e\u3067\u3059\uff1a  \n", "\n", "| \u65b9\u6cd5| CPU\u6642\u9593|\u5b9f\u6642\u9593|  \n", "| ---------------------- | ----------- | ------------ |\n", "| \u30d7\u30ec\u30fc\u30f3| 7.53\u79d2| 7.99\u79d2|  \n", "| \u30d1\u30e9\u30ec\u30eb\u30de\u30c3\u30d7| 8.30\u79d2| 5.94\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6| 8.60\u79d2| 5.47\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6+\u30d1\u30e9\u30ec\u30eb\u30de\u30c3\u30d7| 8.44\u79d2| 5.23\u79d2|"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ML\u30e2\u30c7\u30eb  \n", "\n", "\u4e0a\u8a18\u306e\u8a08\u7b97\u306f\u3001\u3059\u3079\u3066\u306e\u30d4\u30af\u30bb\u30eb\u5024\u3092\u5408\u8a08\u3059\u308b\u3060\u3051\u3067\u304b\u306a\u308a\u5b89\u4fa1\u3067\u3057\u305f\u3002  \n", "\u3082\u3046\u5c11\u3057\u8907\u96d1\u306a\u3082\u306e(\u52fe\u914d\u8a08\u7b97\u306a\u3069)\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u3069\u3046\u306a\u308a\u307e\u3059\u304b\uff1f"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["def train_simple_model(ds, nepochs):\n", "    model = tf.keras.Sequential([\n", "        tf.keras.layers.Flatten(\n", "            input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n", "        #tf.keras.layers.Dense(32, activation='relu'),\n", "        tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')\n", "    ])\n", "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n", "                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n", "                    from_logits=False),\n", "                metrics=['accuracy'])\n", "    model.fit(ds, epochs=nepochs)"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch 1/2\n", "359/359 [==============================] - 4s 11ms/step - loss: 384.5950 - accuracy: 0.2841\n", "Epoch 2/2\n", "359/359 [==============================] - 4s 11ms/step - loss: 260.8144 - accuracy: 0.3983\n", "CPU times: user 9.12 s, sys: 796 ms, total: 9.91 s\n", "Wall time: 9.39 s\n"]}], "source": ["%%time\n", "ds = create_preproc_dataset_plain(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").batch(1)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch 1/2\n", "359/359 [==============================] - 4s 10ms/step - loss: 378.1755 - accuracy: 0.2646\n", "Epoch 2/2\n", "359/359 [==============================] - 4s 10ms/step - loss: 277.4931 - accuracy: 0.4067\n", "CPU times: user 9.97 s, sys: 718 ms, total: 10.7 s\n", "Wall time: 8.17 s\n"]}], "source": ["%%time\n", "# parallel map\n", "ds = create_preproc_dataset_parallelmap(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").batch(1)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "359/359 [==============================] - 3s 9ms/step - loss: 359.4355 - accuracy: 0.3008\n", "Epoch 2/2\n", "359/359 [==============================] - 3s 9ms/step - loss: 296.0292 - accuracy: 0.3928\n", "CPU times: user 9.7 s, sys: 825 ms, total: 10.5 s\n", "Wall time: 7.54 s\n"]}], "source": ["%%time\n", "# with interleave\n", "ds = create_preproc_dataset_interleave(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n", "    num_parallel=None\n", ").batch(1)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "359/359 [==============================] - 3s 9ms/step - loss: 403.3262 - accuracy: 0.2423\n", "Epoch 2/2\n", "359/359 [==============================] - 3s 8ms/step - loss: 260.8356 - accuracy: 0.4290\n", "CPU times: user 9.6 s, sys: 728 ms, total: 10.3 s\n", "Wall time: 7.17 s\n"]}], "source": ["%%time\n", "# with interleave and parallel mpas\n", "ds = create_preproc_dataset_interleave(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX,\n", "    num_parallel=AUTOTUNE\n", ").batch(1)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u6539\u5584\u304c\u6b8b\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002  \n", "\n", "| \u65b9\u6cd5| CPU\u6642\u9593|\u5b9f\u6642\u9593|  \n", "| -----------------------| ----------- | ------------ |\n", "| \u30d7\u30ec\u30fc\u30f3| 9.91\u79d2| 9.39\u79d2|  \n", "| \u30d1\u30e9\u30ec\u30eb\u30de\u30c3\u30d7| 10.7\u79d2| 8.17\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6| 10.5\u79d2| 7.54\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6+\u30d1\u30e9\u30ec\u30eb\u30de\u30c3\u30d7| 10.3\u79d2| 7.17\u79d2|"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u30c7\u30fc\u30bf\u51e6\u7406\u306e\u9ad8\u901f\u5316"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# alias to the more efficient one\n", "def create_preproc_dataset(pattern):\n", "    return create_preproc_dataset_interleave(pattern, num_parallel=AUTOTUNE)"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "359/359 [==============================] - 3s 9ms/step - loss: 387.4389 - accuracy: 0.2813\n", "Epoch 2/2\n", "359/359 [==============================] - 3s 9ms/step - loss: 279.8046 - accuracy: 0.3760\n", "CPU times: user 10.5 s, sys: 899 ms, total: 11.4 s\n", "Wall time: 8.09 s\n"]}], "source": ["%%time\n", "# add prefetching\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").prefetch(AUTOTUNE).batch(1)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "45/45 [==============================] - 3s 60ms/step - loss: 113.6279 - accuracy: 0.2702\n", "Epoch 2/2\n", "45/45 [==============================] - 3s 56ms/step - loss: 120.0435 - accuracy: 0.2758\n", "CPU times: user 8.97 s, sys: 590 ms, total: 9.56 s\n", "Wall time: 6.9 s\n"]}], "source": ["%%time\n", "# Add batching of different sizes\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").prefetch(AUTOTUNE).batch(8)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "23/23 [==============================] - 3s 110ms/step - loss: 97.3240 - accuracy: 0.2507\n", "Epoch 2/2\n", "23/23 [==============================] - 2s 105ms/step - loss: 41.4950 - accuracy: 0.3900\n", "CPU times: user 8.97 s, sys: 939 ms, total: 9.9 s\n", "Wall time: 6.7 s\n"]}], "source": ["%%time\n", "# Add batching of different sizes\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").prefetch(AUTOTUNE).batch(16)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "12/12 [==============================] - 2s 187ms/step - loss: 121.8208 - accuracy: 0.2423\n", "Epoch 2/2\n", "12/12 [==============================] - 2s 185ms/step - loss: 51.0977 - accuracy: 0.3259\n", "CPU times: user 8.78 s, sys: 902 ms, total: 9.68 s\n", "Wall time: 6.37 s\n"]}], "source": ["%%time\n", "# Add batching of different sizes\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").prefetch(AUTOTUNE).batch(32)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "12/12 [==============================] - 3s 213ms/step - loss: 106.7971 - accuracy: 0.2423\n", "Epoch 2/2\n", "12/12 [==============================] - 1s 47ms/step - loss: 66.2376 - accuracy: 0.3203\n", "CPU times: user 5.16 s, sys: 1 s, total: 6.16 s\n", "Wall time: 4.36 s\n"]}], "source": ["%%time\n", "# add caching: always do this optimization last.\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").cache().batch(32)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "12/12 [==============================] - 2s 192ms/step - loss: 126.8334 - accuracy: 0.2340\n", "Epoch 2/2\n", "12/12 [==============================] - 1s 45ms/step - loss: 51.4512 - accuracy: 0.3287\n", "CPU times: user 5.01 s, sys: 756 ms, total: 5.76 s\n", "Wall time: 4.04 s\n"]}], "source": ["%%time\n", "# add caching: always do this optimization last.\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").prefetch(AUTOTUNE).cache().batch(32)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Interleaving the reading of 2 files.\n", "Epoch 1/2\n", "12/12 [==============================] - 2s 208ms/step - loss: 148.2066 - accuracy: 0.2507\n", "Epoch 2/2\n", "12/12 [==============================] - 1s 44ms/step - loss: 71.6666 - accuracy: 0.3064\n", "CPU times: user 4.95 s, sys: 692 ms, total: 5.65 s\n", "Wall time: 4.19 s\n"]}], "source": ["%%time\n", "# add caching: always do this optimization last.\n", "ds = create_preproc_dataset(\n", "    'gs://practical-ml-vision-book/flowers_tfr/train' + PATTERN_SUFFIX\n", ").cache().prefetch(AUTOTUNE).batch(32)\n", "train_simple_model(ds, NUM_EPOCHS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u524d\u306e\u8868\u306b\u8ffd\u52a0\uff1a  \n", "\n", "| \u65b9\u6cd5| CPU\u6642\u9593|\u5b9f\u6642\u9593|  \n", "| -----------------------| ----------- | ------------ |\n", "| \u30d7\u30ec\u30fc\u30f3| 9.91\u79d2| 9.39\u79d2|  \n", "| \u30d1\u30e9\u30ec\u30eb\u30de\u30c3\u30d7| 10.7\u79d2| 8.17\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6| 10.5\u79d2| 7.54\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6+\u30d1\u30e9\u30ec\u30eb\u30de\u30c3\u30d7| 10.3\u79d2| 7.17\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6+\u30d1\u30e9\u30ec\u30eb\u3001\u305d\u3057\u3066\u8ffd\u52a0\uff1a| -| -|  \n", "| \u30d7\u30ea\u30d5\u30a7\u30c3\u30c1| 11.4\u79d2| 8.09\u79d2|  \n", "| \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba8 | 9.56\u79d2| 6\u300290\u5e74\u4ee3|  \n", "| \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba16 | 9\u300290\u5e74\u4ee3| 6\u300270\u5e74\u4ee3|  \n", "| \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba32 | 9.68\u79d2| 6.37\u79d2|  \n", "| \u30a4\u30f3\u30bf\u30fc\u30ea\u30fc\u30d6+\u30d1\u30e9\u30ec\u30eb+\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba32\u3001\u6b21\u306b\u8ffd\u52a0\uff1a| -| -|  \n", "| \u30ad\u30e3\u30c3\u30b7\u30e5| 6.16\u79d2| 4.36\u79d2|  \n", "| \u30d7\u30ea\u30d5\u30a7\u30c3\u30c1+\u30ad\u30e3\u30c3\u30b7\u30e5| 5.76\u79d2| 4.04\u79d2|  \n", "| \u30ad\u30e3\u30c3\u30b7\u30e5+\u30d7\u30ea\u30d5\u30a7\u30c3\u30c1| 5.65\u79d2| 4.19\u79d2|  \n", "\n", "\u3057\u305f\u304c\u3063\u3066\u3001\u6700\u826f\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u306f\u6b21\u306e\u3068\u304a\u308a\u3067\u3059\u3002  \n", "<pre>  \n", "ds = create_preproc_dataset_interleave(pattern\u3001num_parallel = AUTOTUNE).prefetch(AUTOTUNE).cache()\u3002batch(32)  \n", "</pre>"]}, {"cell_type": "markdown", "metadata": {"id": "Duu8mX3iXANE"}, "source": ["## \u30e9\u30a4\u30bb\u30f3\u30b9  \n", "Copyright 2020 Google Inc. Apache License\u30d0\u30fc\u30b8\u30e7\u30f32.0(\u300c\u30e9\u30a4\u30bb\u30f3\u30b9\u300d)\u306b\u57fa\u3065\u3044\u3066\u30e9\u30a4\u30bb\u30f3\u30b9\u4f9b\u4e0e\u3055\u308c\u307e\u3059\u3002\u30e9\u30a4\u30bb\u30f3\u30b9\u306b\u6e96\u62e0\u3059\u308b\u5834\u5408\u3092\u9664\u304d\u3001\u3053\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u307e\u305b\u3093\u3002\u30e9\u30a4\u30bb\u30f3\u30b9\u306e\u30b3\u30d4\u30fc\u306fhttp://www.apache.org/licenses/LICENSE-2.0\u3067\u5165\u624b\u3067\u304d\u307e\u3059\u3002\u9069\u7528\u6cd5\u3067\u8981\u6c42\u3055\u308c\u3066\u3044\u308b\u304b\u3001\u66f8\u9762\u3067\u5408\u610f\u3055\u308c\u3066\u3044\u306a\u3044\u9650\u308a\u3001\u30e9\u30a4\u30bb\u30f3\u30b9\u306b\u57fa\u3065\u3044\u3066\u914d\u5e03\u3055\u308c\u308b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306f\u300c\u73fe\u72b6\u6709\u59ff\u300d\u3067\u914d\u5e03\u3055\u308c\u307e\u3059\u3002\u660e\u793a\u307e\u305f\u306f\u9ed9\u793a\u3092\u554f\u308f\u305a\u3001\u3044\u304b\u306a\u308b\u7a2e\u985e\u306e\u4fdd\u8a3c\u307e\u305f\u306f\u6761\u4ef6\u3082\u3042\u308a\u307e\u305b\u3093\u3002\u30e9\u30a4\u30bb\u30f3\u30b9\u306b\u57fa\u3065\u304f\u8a31\u53ef\u3068\u5236\u9650\u3092\u898f\u5b9a\u3059\u308b\u7279\u5b9a\u306e\u8a00\u8a9e\u306b\u3064\u3044\u3066\u306f\u3001\u30e9\u30a4\u30bb\u30f3\u30b9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": ["5UOm2etrwYCs"], "name": "03a_transfer_learning.ipynb", "provenance": [], "toc_visible": true}, "environment": {"kernel": "python3", "name": "tf2-gpu.2-8.m90", "type": "gcloud", "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m90"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.12"}}, "nbformat": 4, "nbformat_minor": 4}