{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "hiQ6zAoYhyaA",
    "outputId": "0acee878-1207-42c3-9bee-a594acd44365"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?name=Changing+signatures+of+exported+model&download_url=https%3A%2F%2Fgithub.com%2Ftakumiohym%2Fpractical-ml-vision-book-ja%2Fraw%2Fmaster%2F09_deploying%2F09c_changesig.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/09_deploying/09c_changesig.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/09_deploying/09c_changesig.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/09_deploying/09c_changesig.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmt: off\n",
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"09_deploying/09c_changesig.ipynb\"\n",
    "_nb_title = \"Changing signatures of exported model\"\n",
    "\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/takumiohym/practical-ml-vision-book-ja/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/>\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3]))\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# エクスポートされたモデルのSignatureの変更  \n",
    "\n",
    "このノートブックでは、学習済みでエクスポートされたモデルから始めます。  \n",
    "このモデルはパブリックの`gs://practical-ml-vision-book-data/flowers_5_trained`に保存されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定\n",
    "以下に使用するプロジェクト名を入力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'PROJECT_NAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab環境を利用する場合、以下で必要なファイルのダウンロード、プロジェクトの設定を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IS_COLAB_BACKEND = 'COLAB_RELEASE_TAG' in os.environ  # this is always set on Colab\n",
    "if IS_COLAB_BACKEND:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "    # Get files\n",
    "    BASE_PATH = \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/main/09_deploying\"\n",
    "    !wget {BASE_PATH}/vertex_deploy.sh\n",
    "    !sudo chmod 755 ./vertex_deploy.sh\n",
    "\n",
    "    !gcloud config set project {PROJECT}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storageバケットの作成\n",
    "以下のBUCKETの値を使用可能なGSCバケット名に変更して進めてください。<br>\n",
    "使用できるバケットがない場合は、以下の二行目のコマンドをコメントアウトし、gsutil mbコマンドを実行して作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='BUCKET_NAME' # Specify your GCS bucket name\n",
    "# !gsutil mb -l us-central1 -p {PROJECT} gs://{BUCKET} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エクスポートされたモデルを読み込みます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOCATION='gs://practical-ml-vision-book-data/flowers_5_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://practical-ml-vision-book-data/flowers_5_trained/saved_model.pb\n",
      "gs://practical-ml-vision-book-data/flowers_5_trained/chkpts/\n",
      "gs://practical-ml-vision-book-data/flowers_5_trained/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:27:44.322073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:27:44.322125: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['filenames'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_filenames:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['flower_type_int'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['flower_type_str'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:1\n",
      "  outputs['probability'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:2\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir {MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パススルーパラメータを追加する  \n",
    "\n",
    "上記のSignatureは、入力に利用したファイル名を出力しません。パススルーパラメータとして追加してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:27:47.488458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:27:47.488510: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:27:49.654406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:27:49.654453: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-17 21:27:49.654480: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-direct): /proc/driver/nvidia/version does not exist\n",
      "2022-07-17 21:27:49.654756: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-17 21:27:57.425630: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/flowers_model/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(MODEL_LOCATION)\n",
    "old_fn = model.signatures['serving_default']\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def predict_flower_type(filenames):\n",
    "    result = old_fn(filenames) # has flower_type_int etc.\n",
    "    result['filename'] = filenames\n",
    "    return result\n",
    "\n",
    "shutil.rmtree('export', ignore_errors=True)\n",
    "os.mkdir('export')\n",
    "model.save('export/flowers_model',\n",
    "          signatures={\n",
    "              'serving_default': predict_flower_type\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:28:01.865116: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:28:01.865166: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['filenames'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_filenames:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['filename'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['flower_type_int'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:1\n",
      "  outputs['flower_type_str'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:2\n",
      "  outputs['probability'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:3\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --signature_def serving_default --dir export/flowers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([0.61915255, 0.9999844 , 0.99508286, 0.9751858 , 0.954917  ],\n",
      "      dtype=float32)>, 'flower_type_str': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'dandelion', b'dandelion', b'daisy', b'tulips', b'tulips'],\n",
      "      dtype=object)>, 'filename': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg',\n",
      "       b'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg',\n",
      "       b'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg',\n",
      "       b'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg',\n",
      "       b'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg'],\n",
      "      dtype=object)>, 'flower_type_int': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 0, 4, 4])>}\n"
     ]
    }
   ],
   "source": [
    "serving_fn = tf.keras.models.load_model('export/flowers_model').signatures['serving_default']\n",
    "filenames = [\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg'\n",
    "]\n",
    "pred = serving_fn(tf.convert_to_tensor(filenames))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力に、入力したファイル名が合わせて出力されていることが確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数のSignature\n",
    "\n",
    "続けて、複数のSignatureを定義する方法を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/flowers_model2/assets\n"
     ]
    }
   ],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def pass_through_input(filenames):\n",
    "    result = old_fn(filenames) # has flower_type_int etc.\n",
    "    result['filename'] = filenames\n",
    "    return result\n",
    "\n",
    "shutil.rmtree('export', ignore_errors=True)\n",
    "os.mkdir('export')\n",
    "model.save('export/flowers_model2',\n",
    "          signatures={\n",
    "              'serving_default': old_fn,\n",
    "              'input_pass_through': pass_through_input\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:28:17.227397: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:28:17.227450: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"input_pass_through\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir export/flowers_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`serving_default`シグネチャは先程と同様です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:28:20.193490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:28:20.193548: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['filenames'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_filenames:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['flower_type_int'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:0\n",
      "  outputs['flower_type_str'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:1\n",
      "  outputs['probability'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:2\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir export/flowers_model2 --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "２つ目の`input_pass_through`シグネチャを確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:28:23.166491: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:28:23.166543: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['filenames'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: input_pass_through_filenames:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['filename'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['flower_type_int'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:1\n",
      "  outputs['flower_type_str'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:2\n",
      "  outputs['probability'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:3\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir export/flowers_model2 --signature_def input_pass_through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数のSignatureを持つモデルをREST APIとしてデプロイ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./export/flowers_model2/keras_metadata.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://./export/flowers_model2/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://./export/flowers_model2/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "Copying file://./export/flowers_model2/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "/ [4/4 files][ 11.0 MiB/ 11.0 MiB] 100% Done                                    \n",
      "Operation completed over 4 objects/11.0 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r ./export/flowers_model2 gs://{BUCKET}/flowers_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルをデプロイします。この処理には10分ほど時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model multi\n",
      "The endpoint named multi already exists.\n",
      "The endpoint_id is 7785553875351633920\n",
      "The model named multi already exists.\n",
      "The model_id is 6103305482900865024\n",
      "Deploying model now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [1042486075676688384]...\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Deployed a model to the endpoint 7785553875351633920. Id of the deployed model: 4075691692072632320.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./vertex_deploy.sh \\\n",
    "--endpoint_name=multi \\\n",
    "--model_name=multi \\\n",
    "--model_location=gs://${BUCKET}/flowers_model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下のセルを変更する\n",
    "上記のコマンドから出力された`endpoint_id`を下記にコピーしてください。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THESE TO REFLECT WHERE YOU DEPLOYED THE MODEL\n",
    "os.environ['ENDPOINT_ID'] = '7785553875351633920' # CHANGE\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `serving_default`の呼び出し\n",
    "`serving_default`を呼び出す場合は、[`09b_rest.ipynb`](https://github.com/takumiohym/practical-ml-vision-book-ja/blob/main/09_deploying/09b_rest.ipynb)と同様に、JSONを定義し、`gcloud ai endpoints predict`コマンドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting request.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile request.json\n",
    "{\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"deployedModelId\": \"4075691692072632320\",\n",
      "  \"model\": \"projects/849204435784/locations/us-central1/models/6103305482900865024\",\n",
      "  \"modelDisplayName\": \"multi\",\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"flower_type_int\": 1,\n",
      "      \"flower_type_str\": \"dandelion\",\n",
      "      \"probability\": 0.619152546\n",
      "    },\n",
      "    {\n",
      "      \"flower_type_int\": 1,\n",
      "      \"flower_type_str\": \"dandelion\",\n",
      "      \"probability\": 0.999984384\n",
      "    },\n",
      "    {\n",
      "      \"flower_type_int\": 0,\n",
      "      \"flower_type_str\": \"daisy\",\n",
      "      \"probability\": 0.995082855\n",
      "    },\n",
      "    {\n",
      "      \"flower_type_int\": 4,\n",
      "      \"flower_type_str\": \"tulips\",\n",
      "      \"probability\": 0.975185812\n",
      "    },\n",
      "    {\n",
      "      \"flower_type_int\": 4,\n",
      "      \"flower_type_str\": \"tulips\",\n",
      "      \"probability\": 0.954917\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai endpoints predict ${ENDPOINT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--json-request=request.json \\\n",
    "--format=json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ２つ目以降のシグネチャの利用\n",
    "`serving_default`以外のシグネチャを利用する場合以下のことを行う必要があります。\n",
    "\n",
    "- JSONファイルに`signature_name`キーを追加\n",
    "- `gcloud ai endpoints raw-predict`コマンドで呼び出し\n",
    "\n",
    "現在Vertex AIでは、`gcloud ai endpoints predict`ではなく、`gcloud ai endpoints raw-predict`コマンドで`signature_name`キーをサポートしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting request.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile request.json\n",
    "{\n",
    "    \"signature_name\": \"input_pass_through\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [\n",
      "        {\n",
      "            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg\",\n",
      "            \"flower_type_str\": \"dandelion\",\n",
      "            \"probability\": 0.619152546,\n",
      "            \"flower_type_int\": 1\n",
      "        },\n",
      "        {\n",
      "            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg\",\n",
      "            \"flower_type_str\": \"dandelion\",\n",
      "            \"probability\": 0.999984384,\n",
      "            \"flower_type_int\": 1\n",
      "        },\n",
      "        {\n",
      "            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg\",\n",
      "            \"flower_type_str\": \"daisy\",\n",
      "            \"probability\": 0.995082855,\n",
      "            \"flower_type_int\": 0\n",
      "        },\n",
      "        {\n",
      "            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg\",\n",
      "            \"flower_type_str\": \"tulips\",\n",
      "            \"probability\": 0.975185812,\n",
      "            \"flower_type_int\": 4\n",
      "        },\n",
      "        {\n",
      "            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg\",\n",
      "            \"flower_type_str\": \"tulips\",\n",
      "            \"probability\": 0.954917,\n",
      "            \"flower_type_int\": 4\n",
      "        }\n",
      "    ]\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai endpoints raw-predict ${ENDPOINT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--request=@request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、同様のことは、直接HTTPリクエストを送信する方法でも実行可能です。<br>\n",
    "[`09b_rest.ipynb`](https://github.com/takumiohym/practical-ml-vision-book-ja/blob/main/09_deploying/09b_rest.ipynb)と異なり、URLの最後が`rawPredict`となっていることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n    \"predictions\": [\\n        {\\n            \"flower_type_str\": \"dandelion\",\\n            \"probability\": 0.619152546,\\n            \"flower_type_int\": 1,\\n            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg\"\\n        },\\n        {\\n            \"flower_type_str\": \"dandelion\",\\n            \"probability\": 0.999984384,\\n            \"flower_type_int\": 1,\\n            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg\"\\n        },\\n        {\\n            \"flower_type_str\": \"daisy\",\\n            \"probability\": 0.995082855,\\n            \"flower_type_int\": 0,\\n            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg\"\\n        },\\n        {\\n            \"flower_type_str\": \"tulips\",\\n            \"probability\": 0.975185812,\\n            \"flower_type_int\": 4,\\n            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg\"\\n        },\\n        {\\n            \"flower_type_str\": \"tulips\",\\n            \"probability\": 0.954917,\\n            \"flower_type_int\": 4,\\n            \"filename\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg\"\\n        }\\n    ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "# Invoke from Python.\n",
    "import json\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import requests\n",
    "\n",
    "PROJECT = os.environ['PROJECT']\n",
    "REGION = os.environ['REGION']\n",
    "ENDPOINT_ID = os.environ['ENDPOINT_ID']\n",
    "\n",
    "token = GoogleCredentials.get_application_default().get_access_token().access_token\n",
    "api = \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:rawPredict\".format(\n",
    "    REGION, PROJECT, REGION, ENDPOINT_ID)\n",
    "headers = {\"Authorization\": \"Bearer \" + token }\n",
    "data = {\n",
    "    \"signature_name\": \"input_pass_through\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg\"\n",
    "        },\n",
    "        {\n",
    "            \"filenames\": \"gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "response = requests.post(api, json=data, headers=headers)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu8mX3iXANE"
   },
   "source": [
    "## License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "09c_changesig.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
