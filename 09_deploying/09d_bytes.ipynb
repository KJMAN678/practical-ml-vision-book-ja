{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "hiQ6zAoYhyaA",
    "outputId": "0acee878-1207-42c3-9bee-a594acd44365"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?name=Handling+image+bytes&download_url=https%3A%2F%2Fgithub.com%2Ftakumiohym%2Fpractical-ml-vision-book-ja%2Fraw%2Fmaster%2F09_deploying%2F09d_bytes.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/09_deploying/09d_bytes.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/09_deploying/09d_bytes.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/09_deploying/09d_bytes.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmt: off\n",
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"09_deploying/09d_bytes.ipynb\"\n",
    "_nb_title = \"Handling image bytes\"\n",
    "\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/takumiohym/practical-ml-vision-book-ja/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/>\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3]))\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8HQYsAtC0Fv"
   },
   "source": [
    "# 画像バイトの処理  \n",
    "\n",
    "このノートブックでは、学習済みでエクスポートされたモデルから始めます。  \n",
    "このモデルはパブリックの`gs://practical-ml-vision-book-data/flowers_5_trained`に保存されています。\n",
    "\n",
    "ここでは、ネットワークを介してバイトを直接処理するようにモデルを変更します。そうすることで、クライアントは画像をGoogle Cloud Storageに転送する必要がなくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定\n",
    "以下に使用するプロジェクト名を入力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'PROJECT_NAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab環境を利用する場合、以下で必要なファイルのダウンロード、プロジェクトの設定を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IS_COLAB_BACKEND = 'COLAB_RELEASE_TAG' in os.environ  # this is always set on Colab\n",
    "if IS_COLAB_BACKEND:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "    # Get files\n",
    "    BASE_PATH = \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/main/09_deploying\"\n",
    "    !wget {BASE_PATH}/vertex_deploy.sh\n",
    "    !sudo chmod 755 ./vertex_deploy.sh\n",
    "\n",
    "    !gcloud config set project {PROJECT}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storageバケットの作成\n",
    "以下のBUCKETの値を使用可能なGSCバケット名に変更して進めてください。\n",
    "使用できるバケットがない場合は、以下の二行目のコマンドをコメントアウトし、gsutil mbコマンドを実行して作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='BUCKET_NAME' # Specify your GCS bucket name\n",
    "# !gsutil mb -l us-central1 -p {PROJECT} gs://{BUCKET} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読み込み\n",
    "エクスポートされたモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:29:10.827657: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:29:10.827711: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:29:13.014347: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:29:13.014390: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-17 21:29:13.014415: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-direct): /proc/driver/nvidia/version does not exist\n",
      "2022-07-17 21:29:13.014664: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"flower_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random/center_crop (RandomC  (None, 224, 224, 3)      0         \n",
      " rop)                                                            \n",
      "                                                                 \n",
      " random_lr_flip/none (Random  (None, 224, 224, 3)      0         \n",
      " Flip)                                                           \n",
      "                                                                 \n",
      " mobilenet_embedding (KerasL  (None, 1280)             2257984   \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dense_hidden (Dense)        (None, 32)                40992     \n",
      "                                                                 \n",
      " flower_prob (Dense)         (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,299,141\n",
      "Trainable params: 41,157\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "MODEL_LOCATION='gs://practical-ml-vision-book-data/flowers_5_trained'\n",
    "model = tf.keras.models.load_model(MODEL_LOCATION)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 345\n",
    "IMG_WIDTH = 345\n",
    "IMG_CHANNELS = 3\n",
    "CLASS_NAMES = 'daisy dandelion roses sunflowers tulips'.split()\n",
    "\n",
    "def read_from_jpegfile(filename):\n",
    "    img_bytes = tf.io.read_file(filename)\n",
    "    return img_bytes\n",
    "    \n",
    "def preprocess(img_bytes):\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.image.resize_with_pad(img, IMG_HEIGHT, IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20596564 0.61915255 0.0110638  0.08623137 0.07758673]]\n",
      "[[1.3109286e-05 9.9998438e-01 2.5930876e-07 1.2337833e-06 1.0490534e-06]]\n",
      "[[9.9508286e-01 3.4755392e-03 7.9892470e-06 1.4052965e-03 2.8355907e-05]]\n",
      "[[2.8463281e-03 4.0272961e-04 1.6168093e-02 5.3968919e-03 9.7518593e-01]]\n",
      "[[3.5318535e-06 7.0068648e-07 4.5077931e-02 8.6750697e-07 9.5491701e-01]]\n"
     ]
    }
   ],
   "source": [
    "filenames = [\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9818247_e2eac18894.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/dandelion/9853885425_4a82356f1d_m.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8733586143_3139db6e9e_n.jpg',\n",
    "    'gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg'\n",
    "]\n",
    "for filename in filenames:\n",
    "    img_bytes = read_from_jpegfile(filename)\n",
    "    img = preprocess(img_bytes)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    pred = model.predict(img)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バイト列を処理するシグネチャのエクスポート\n",
    "ファイル名でなく、モデルに直接画像ファイルを送れるように、画像のバイト列を処理することのできるシグネチャを追加してエクスポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:29:28.725077: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/flowers_model3/assets\n"
     ]
    }
   ],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def predict_bytes(img_bytes):\n",
    "    input_images = tf.map_fn(\n",
    "        preprocess,\n",
    "        img_bytes,\n",
    "        fn_output_signature=tf.float32\n",
    "    )\n",
    "    batch_pred = model(input_images) # same as model.predict()\n",
    "    top_prob = tf.math.reduce_max(batch_pred, axis=[1])\n",
    "    pred_label_index = tf.math.argmax(batch_pred, axis=1)\n",
    "    pred_label = tf.gather(tf.convert_to_tensor(CLASS_NAMES), pred_label_index)\n",
    "    return {\n",
    "        'probability': top_prob,\n",
    "        'flower_type_int': pred_label_index,\n",
    "        'flower_type_str': pred_label\n",
    "    }\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None,], dtype=tf.string)])\n",
    "def predict_filename(filenames):\n",
    "    img_bytes = tf.map_fn(\n",
    "        tf.io.read_file,\n",
    "        filenames\n",
    "    )\n",
    "    result = predict_bytes(img_bytes)\n",
    "    result['filename'] = filenames\n",
    "    return result\n",
    "\n",
    "shutil.rmtree('export', ignore_errors=True)\n",
    "os.mkdir('export')\n",
    "model.save('export/flowers_model3',\n",
    "          signatures={\n",
    "              'serving_default': predict_filename,\n",
    "              'from_bytes': predict_bytes\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:29:33.095800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:29:33.095853: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"from_bytes\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir export/flowers_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`serving_default`シグネチャには変更はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:29:36.165361: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:29:36.165415: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['filenames'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_filenames:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['filename'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:0\n",
      "  outputs['flower_type_int'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:1\n",
      "  outputs['flower_type_str'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:2\n",
      "  outputs['probability'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall_1:3\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir export/flowers_model3 --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新たに追加した`from_bytes`シグネチャを確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-17 21:29:39.238504: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 21:29:39.238556: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['img_bytes'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: from_bytes_img_bytes:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['flower_type_int'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "  outputs['flower_type_str'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:1\n",
      "  outputs['probability'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1)\n",
      "      name: StatefulPartitionedCall:2\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --tag_set serve --dir export/flowers_model3 --signature_def from_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imgバイトをネットワーク経由で送信\n",
    "\n",
    "新しいシグネチャを利用することで、GCSに中間ファイルを置く必要はなくなります。<br>\n",
    "画像データをローカルにコピーし、直接推論を実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg...\n",
      "/ [1 files][ 19.4 KiB/ 19.4 KiB]                                                \n",
      "Operation completed over 1 objects/19.4 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg /tmp/test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flower_type_int': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, 'flower_type_str': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'daisy'], dtype=object)>, 'probability': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.99508286], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "with open('/tmp/test.jpg', 'rb') as ifp:\n",
    "    img_bytes = ifp.read()\n",
    "    serving_fn = tf.keras.models.load_model('./export/flowers_model3').signatures['from_bytes']\n",
    "    pred = serving_fn(tf.convert_to_tensor([img_bytes]))\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Prediction へデプロイ\n",
    "\n",
    "新しいシグネチャを持つモデルをVertex AI Predictionへデプロイします。\n",
    "\n",
    "デプロイには10分ほどの時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./export/flowers_model3/keras_metadata.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://./export/flowers_model3/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://./export/flowers_model3/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "Copying file://./export/flowers_model3/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "/ [4/4 files][ 11.0 MiB/ 11.0 MiB] 100% Done                                    \n",
      "Operation completed over 4 objects/11.0 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil -m cp -r ./export/flowers_model3 gs://${BUCKET}/flowers_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model bytes\n",
      "Creating bytes endpoint now.\n",
      "The endpoint_id is 8247172837157109760\n",
      "Uploading bytes model now.\n",
      "The model_id is 953439309002702848\n",
      "Deploying model now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [2900220921967017984]...\n",
      ".....done.\n",
      "Created Vertex AI endpoint: projects/849204435784/locations/us-central1/endpoints/8247172837157109760.\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [8200957683382091776]...\n",
      ".......................................done.\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [7960015103317770240]...\n",
      "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Deployed a model to the endpoint 8247172837157109760. Id of the deployed model: 8687377710500020224.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./vertex_deploy.sh \\\n",
    "--endpoint_name=bytes \\\n",
    "--model_name=bytes \\\n",
    "--model_location=gs://${BUCKET}/flowers_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下のセルを変更する\n",
    "上記のコマンドから出力された`endpoint_id`を下記にコピーしてください。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THESE TO REFLECT WHERE YOU DEPLOYED THE MODEL\n",
    "import os\n",
    "os.environ['ENDPOINT_ID'] = '8247172837157109760' # CHANGE\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg...\n",
      "/ [1 files][ 19.4 KiB/ 19.4 KiB]                                                \n",
      "Operation completed over 1 objects/19.4 KiB.                                     \n",
      "Copying gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg...\n",
      "/ [1 files][ 34.6 KiB/ 34.6 KiB]                                                \n",
      "Operation completed over 1 objects/34.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/daisy/9299302012_958c70564c_n.jpg /tmp/test1.jpg\n",
    "gsutil cp gs://practical-ml-vision-book-data/flowers_5_jpeg/flower_photos/tulips/8713397358_0505cc0176_n.jpg /tmp/test2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTPリクエスト\n",
    "新しい`from_bytes`シグネチャにHTTPリクエストを送ります。\n",
    "\n",
    "Base64でエンコードされたデータを渡す方法に注意してください。<br>\n",
    "`img_bytes`キーの下に`b64`キーを定義し、バイト列を値として渡しています。こうすることで、Predictionに使用するTF Servingはこの値を自動的にデコードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n    \"predictions\": [\\n        {\\n            \"flower_type_str\": \"daisy\",\\n            \"probability\": 0.995082855,\\n            \"flower_type_int\": 0\\n        },\\n        {\\n            \"flower_type_str\": \"tulips\",\\n            \"probability\": 0.954917,\\n            \"flower_type_int\": 4\\n        }\\n    ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "# Invoke from Python.\n",
    "import base64\n",
    "import json\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import requests\n",
    "\n",
    "PROJECT = os.environ['PROJECT']\n",
    "REGION = os.environ['REGION']\n",
    "ENDPOINT_ID = os.environ['ENDPOINT_ID']\n",
    "\n",
    "def b64encode(filename):\n",
    "    with open(filename, 'rb') as ifp:\n",
    "        img_bytes = ifp.read()\n",
    "        return base64.b64encode(img_bytes).decode()\n",
    "\n",
    "token = GoogleCredentials.get_application_default().get_access_token().access_token\n",
    "api = \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/endpoints/{}:rawPredict\".format(\n",
    "    REGION, PROJECT, REGION, ENDPOINT_ID)\n",
    "headers = {\"Authorization\": \"Bearer \" + token }\n",
    "data = {\n",
    "    \"signature_name\": \"from_bytes\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"img_bytes\": {\"b64\": b64encode('/tmp/test1.jpg')}\n",
    "        },\n",
    "        {\n",
    "            \"img_bytes\": {\"b64\": b64encode('/tmp/test2.jpg')}\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "response = requests.post(api, json=data, headers=headers)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duu8mX3iXANE"
   },
   "source": [
    "## License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5UOm2etrwYCs"
   ],
   "name": "09d_bytes.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
