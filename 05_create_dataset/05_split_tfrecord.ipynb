{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "0uUeDqA32K9o",
    "outputId": "27b66765-ee49-4504-f32e-f34776c4f3b4"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?name=Splitting+dataset+and+writing+TF+Records&download_url=https%3A%2F%2Fgithub.com%2Ftakumiohym%2Fpractical-ml-vision-book-ja%2Fraw%2Fmaster%2F05_create_dataset%2F05_split_tfrecord.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/05_create_dataset/05_split_tfrecord.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/05_create_dataset/05_split_tfrecord.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/05_create_dataset/05_split_tfrecord.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "### change to reflect your notebook\n",
    "_nb_loc = \"05_create_dataset/05_split_tfrecord.ipynb\"\n",
    "_nb_title = \"Splitting dataset and writing TF Records\"\n",
    "\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/takumiohym/practical-ml-vision-book-ja/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://github.com/takumiohym/practical-ml-vision-book-ja/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in Vertex AI Workbench</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/>\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gksy_Cqe2PND"
   },
   "source": [
    "# データセットの分割とTFRecordsの書き込み  \n",
    "\n",
    "このノートブックでは、データセットを学習用、検証用、テスト用に分割し、それらの画像をTensorFlow Recordsファイルに書き込む方法を示します。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colabを利用する場合、以下のコマンドでアカウントの認証、環境設定を行う必要があります。<br>\n",
    "実行には時間がかかりますので、可能であればVertex AI Workbenchでの実行をおすすめします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q apache-beam==2.38.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab環境を利用する場合、以下を実行して必要なモジュールのインストール、ファイルのダウンロードを行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB_BACKEND = 'COLAB_RELEASE_TAG' in os.environ  # this is always set on Colab\n",
    "if IS_COLAB_BACKEND:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install gcsfs\n",
    "    !pip install apache-beam[gcp]\n",
    "\n",
    "    # Get files\n",
    "    BASE_PATH = \"https://raw.githubusercontent.com/takumiohym/practical-ml-vision-book-ja/main/05_create_dataset\"\n",
    "    !wget {BASE_PATH}/jpeg_to_tfrecord.py\n",
    "    !wget {BASE_PATH}/run_dataflow.sh\n",
    "    !sudo chmod 755 jpeg_to_tfrecord.py\n",
    "    !sudo chmod 755 run_dataflow.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のセルの`PROJECT`にGoogle CloudのプロジェクトID、`BUCKET`に利用可能なGoogle Cloud Storageバケット名を入力してください。\n",
    "\n",
    "また、使用できるバケットがない場合は、以下の三行目のコマンドをコメントアウトし、gsutil mbコマンドを実行して作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"PROJECT_NAME\"\n",
    "BUCKET = \"BUCKET_NAME\"\n",
    "# !gsutil mb -l us-central1 -p {PROJECT} gs://{BUCKET} \n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://practical-ml-vision-book/flowers_5_jpeg/f...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs://practical-ml-vision-book/flowers_5_jpeg/f...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs://practical-ml-vision-book/flowers_5_jpeg/f...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gs://practical-ml-vision-book/flowers_5_jpeg/f...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gs://practical-ml-vision-book/flowers_5_jpeg/f...</td>\n",
       "      <td>daisy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  gs://practical-ml-vision-book/flowers_5_jpeg/f...  daisy\n",
       "1  gs://practical-ml-vision-book/flowers_5_jpeg/f...  daisy\n",
       "2  gs://practical-ml-vision-book/flowers_5_jpeg/f...  daisy\n",
       "3  gs://practical-ml-vision-book/flowers_5_jpeg/f...  daisy\n",
       "4  gs://practical-ml-vision-book/flowers_5_jpeg/f...  daisy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/all_data.csv', names=['image','label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのランダムな分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670 2930 359 381\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "rnd = np.random.rand(len(df))\n",
    "train = df[ rnd < 0.8  ]\n",
    "valid = df[ (rnd >= 0.8) & (rnd < 0.9) ]\n",
    "test  = df[ rnd >= 0.9 ]\n",
    "print(len(df), len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf output\n",
    "mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('output/train.csv', header=False, index=False)\n",
    "valid.to_csv('output/valid.csv', header=False, index=False)\n",
    "test.to_csv('output/test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10466290366_cc72e33532.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/11642632_1e7627a2cc.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/13583238844_573df2de8e_m.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/1374193928_a52320eafa.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/13953307149_f8de6a768c_m.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/14471433500_cdaa22e3ea_m.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/14523675369_97c31d0b5b.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/163978992_8128b49d3e_n.jpg,daisy\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/16401288243_36112bd52f_m.jpg,daisy\n"
     ]
    }
   ],
   "source": [
    "!head output/test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beamを使用したTFレコードの書き込み  \n",
    "ここでは、Apache Beamを利用して、jpgの画像データとラベルを読み込み、TFRecords形式に書き込む方法を確認します。\n",
    "\n",
    "まずはローカルで実行するため、速度を優先して、5つのレコードだけを書き込んでみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf = test.head()\n",
    "len(outdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10466290366_cc72e33532.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/11642632_1e7627a2cc.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/13583238844_573df2de8e_m.jpg',\n",
       "        'daisy'],\n",
       "       ['gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/1374193928_a52320eafa.jpg',\n",
       "        'daisy']], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy\n",
      "dandelion\n",
      "roses\n",
      "sunflowers\n",
      "tulips\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 22:42:35.221523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-10 22:42:35.221593: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 5 labels, from daisy to tulips\n"
     ]
    }
   ],
   "source": [
    "with tf.io.gfile.GFile('gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dict.txt', 'r') as f:\n",
    "    LABELS = [line.rstrip() for line in f]\n",
    "print('Read in {} labels, from {} to {}'.format(\n",
    "    len(LABELS), LABELS[0], LABELS[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10466290366_cc72e33532.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 22:42:41.318424: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-10 22:42:41.318473: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-10 22:42:41.318498: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-direct): /proc/driver/nvidia/version does not exist\n",
      "2022-12-10 22:42:41.318851: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/10712722853_5632165b04.jpg\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/11642632_1e7627a2cc.jpg\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/13583238844_573df2de8e_m.jpg\n",
      "gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/daisy/1374193928_a52320eafa.jpg\n"
     ]
    }
   ],
   "source": [
    "def _string_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def read_and_decode(filename):\n",
    "    IMG_CHANNELS = 3\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "def create_tfrecord(filename, label, label_int):\n",
    "    print(filename)\n",
    "    img = read_and_decode(filename)\n",
    "    dims = img.shape\n",
    "    img = tf.reshape(img, [-1]) # flatten to 1D array\n",
    "    return tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image': _float_feature(img),\n",
    "        'shape': _int64_feature([dims[0], dims[1], dims[2]]),\n",
    "        'label': _string_feature(label),\n",
    "        'label_int': _int64_feature([label_int])\n",
    "    })).SerializeToString()\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    (p \n",
    "     | 'input_df' >> beam.Create(outdf.values)\n",
    "     | 'create_tfrecord' >> beam.Map(lambda x: create_tfrecord(x[0], x[1], LABELS.index(x[1])))\n",
    "     | 'write' >> beam.io.tfrecordio.WriteToTFRecord('output/train')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter 8777320 Dec 10 22:52 output/train-00000-of-00001\n",
      "-rw-r--r-- 1 jupyter jupyter  300932 Dec 10 22:42 output/train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l output/train*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beamでのデータの分割\n",
    "Apache Beamを利用したデータ分割のサンプルコードを示します。ここではダミーデータを利用しています。<br>\n",
    "`splits`で定義されたデータを、`train`と`valid`に分割して処理していることに注目してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardcoded:  train a train True\n",
      "hardcoded:  train a valid False\n",
      "hardcoded:  train b train True\n",
      "hardcoded:  train b valid False\n",
      "hardcoded:  valid c train False\n",
      "hardcoded:  valid c valid True\n",
      "hardcoded:  valid d train False\n",
      "hardcoded:  valid d valid True\n"
     ]
    }
   ],
   "source": [
    "def hardcoded(x, desired_split):\n",
    "    split, rec = x\n",
    "    print('hardcoded: ', split, rec, desired_split, split == desired_split)\n",
    "    if split == desired_split:\n",
    "        yield rec\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "        splits = (p\n",
    "                  | 'input_df' >> beam.Create([\n",
    "                      ('train', 'a'),\n",
    "                      ('train', 'b'),\n",
    "                      ('valid', 'c'),\n",
    "                      ('valid', 'd')\n",
    "                  ]))\n",
    "        \n",
    "        split = 'train'\n",
    "        _ = (splits\n",
    "                 | 'h_only_{}'.format(split) >> beam.FlatMap(\n",
    "                     lambda x: hardcoded(x, 'train'))\n",
    "         )        \n",
    "        split = 'valid'\n",
    "        _ = (splits\n",
    "                 | 'h_only_{}'.format(split) >> beam.FlatMap(\n",
    "                     lambda x: hardcoded(x, 'valid'))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflowで実行  \n",
    "\n",
    "Apache Beamコードは、Cloud Dataflowを使用してサーバーレスに実行できます。\n",
    "\n",
    "[`./jpeg_to_tfrecord.py`](https://github.com/takumiohym/practical-ml-vision-book-ja/blob/main/05_create_dataset/jpeg_to_tfrecord.py) にCloud Dataflow用のコードが用意されています。<br>\n",
    "beam.Pipeline() がクラウド上での実行用に以下のように書き換えられていることに注目してください。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "options = {\n",
    "      'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "      'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "      'job_name': JOBNAME,\n",
    "      'project': PROJECT,\n",
    "      'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "      'save_main_session': True\n",
    "  }\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコマンドで、Dataflowのジョブを実行します。\n",
    "\n",
    "Google Cloudのコンソールから、Navigation Menu -> Dataflow -> preprocess-images-<timestamp> へ移動し、実行の様子を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n",
      "Read in 5 labels, from daisy to tulips\n",
      "Submitting DataflowRunner job: preprocess-images-221210-225839\n",
      "Monitor at https://console.cloud.google.com/dataflow/jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 22:58:37.563592: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-10 22:58:37.563652: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CommandException: 1 files/objects could not be removed.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS'}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'teardown_policy': 'TEARDOWN_ALWAYS'}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m jpeg_to_tfrecord \\\n",
    "       --all_data gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/all_data.csv \\\n",
    "       --labels_file gs://practical-ml-vision-book/flowers_5_jpeg/flower_photos/dict.txt \\\n",
    "       --project_id $PROJECT \\\n",
    "       --output_dir gs://${BUCKET}/data/flower_tfrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dataflow_pipeline.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジョブは30分ほどで完了します。\n",
    "\n",
    "実行が完了したら、以下のセルで出力を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls -l gs://${BUCKET}/data/flower_tfrecords/*-00001-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_fNzWuY2UoB"
   },
   "source": [
    "## License\n",
    "Copyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_split_tfrecord.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
